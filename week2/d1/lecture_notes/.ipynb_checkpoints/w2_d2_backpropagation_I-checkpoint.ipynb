{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation I: Autodifferentiation and Computational Graphs \n",
    "\n",
    "### By the end of this lecture you will be able to:\n",
    "1. Describe in your own words what a computational graph is\n",
    "2. Write a computational graph for a small equation\n",
    "3. Solve a computational graph forwards and backwards for derivatives\n",
    "4. Write a computational graph for a NN\n",
    "\n",
    "In our last lecture we explored the concept of backpropagation across a very small network, and this proved to be rather laborious. In the following lectures we are going to learn how it is possible to construct neural networks of theoretically unlimited size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Autodifferentiation\n",
    "\n",
    "Making large networks are certainly possible with the tools you have seen at this point, although it would require that each and every derivative was precomputed symbolically by hand for each layer of the network and then implemented in code. Although this is possible to do, the likelihood that human error will be introduced along the way is high.\n",
    "\n",
    "Fortunately, an algorithm, nearly 50 years in the making now, has come to the forefront that allows us to compute derivatives along the entire network in a single step and furthermore, **automatically.** This is called **automatic differentiation**, or **autodifferentiation.**\n",
    "\n",
    "How is such a thing possible? It begins with the concept of a **computatational graph**, which is a way of abstracting the complexity of a layered neural network:\n",
    "\n",
    "![NN1](./images/bp1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "into a single function represented as a **graph**:\n",
    "![NN2](./images/bp2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is not such a strange idea on its surface, after all, in our last lecture we described the 1-hidden layer MLP as a system of equations (using $\\sigma$ instead of \"f\", to clearly represent the sigmoid function):\n",
    "\n",
    "$$\n",
    "z^{(2)} = XW^{(1)} \\tag{1}\\\\\n",
    "$$\n",
    "$$\n",
    "a^{(2)} = \\sigma(z^{(2)}) \\tag{2}\\\\\n",
    "$$\n",
    "$$\n",
    "z^{(3)} = a^{(2)}W^{(2)} \\tag{3}\\\\\n",
    "$$\n",
    "$$\n",
    "\\hat{y} = \\sigma(z^{(3)}) \\tag{4}\\\\\n",
    "$$\n",
    "$$\n",
    "J = \\sum \\frac{1}{2}(y-\\hat{y})^2 \\tag{5}\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equations 1-4 end up condensing neatly to the following vector form:\n",
    "\n",
    "$$NN({\\bf{x}}, \\theta) = \\sigma(\\sigma({\\bf{x}}{\\bf{W^{(1)}}}){\\bf{W^{(2)}}})$$\n",
    "\n",
    "Following on from this we can also add in bias terms for each layer, $\\theta_1$ and $\\theta_2$.\n",
    "\n",
    "$$NN({\\bf{x}}, \\theta) = \\sigma(\\sigma({\\bf{x}}{\\bf{W^{(1)}}}+\\theta_{1}){\\bf{W^{(2)}}}+\\theta_{2})$$\n",
    "\n",
    "We can go bonkers with this abstraction, adding for example, an entire second hidden layer and third bias term:\n",
    "\n",
    "$$NN({\\bf{x}}, \\theta) = \\sigma(\\sigma(\\sigma({\\bf{x}}{\\bf{W^{(1)}}}+\\theta_{1}){\\bf{W^{(2)}}}+\\theta_{2}){\\bf{W^{(3)}}}+\\theta_{3})$$\n",
    "\n",
    "In the end, we simply take this large composite function, hook it up to a loss function of our choosing, and optimize:\n",
    "\n",
    "![optimization](./images/optimization_with_loss.PNG)\n",
    "\n",
    "Where the forward pass is in blue, the backwards pass is in red, and the update step is in green. But how to efficiently calculate derivatives of that large function? It presents an onerous task, even for only three or four layers. We do so with autodifferentiation, which in turn is done with computational graph theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing a Computational Graph by Pathway Analysis\n",
    "\n",
    "Consider the following function:\n",
    "\n",
    "$$ f(x,y) = \\dfrac{1}{y+e^{-x}} $$\n",
    "\n",
    "How might you expect to write a function in the computer that produces this calculation as a composition of primitive functions? Make sure to give each node a name, and then name the output of that node in terms of what primitive functions it represents.\n",
    "\n",
    "![stepwise graph](./images/stepwise_path_differentiation.PNG)\n",
    "\n",
    "We would probably write code as follows:\n",
    "\n",
    "    def f(x,y):\n",
    "        a = -x\n",
    "        b = math.e ** a\n",
    "        c = y + b\n",
    "        d = 1 /c\n",
    "        return d\n",
    "\n",
    "\n",
    "### Computing $\\dfrac{\\partial i}{\\partial j}$ with reverse differentiation\n",
    "\n",
    "Now the point of the graph is that the calculation that we followed in the above section can be traced from its origin all along the pathway, leading back to the starting point. This means that the calculation should be able to be traced (backward) from any origin node $i$ to any destination node $j$ - in this case we are referring to the node names (\"a\", \"b\", \"c\", \"d\") etc. \n",
    "\n",
    "Now let us consider the following problem:\n",
    "\n",
    "Given $ f(x,y) = \\dfrac{1}{y+e^{-x}} $, consider $f(0,1)$, and compute $\\dfrac{\\partial f}{\\partial y}$.  (The solution for this calculation lies only two steps back on the below graph.)\n",
    "\n",
    "![stepwise graph](./images/stepwise_path_2.PNG)\n",
    "\n",
    "\n",
    "A) On the output node (\"f\"), $\\dfrac{\\partial d}{\\partial d} = 1$ (this term is always 1)\n",
    "\n",
    "B) We must step back once from $d$ (the output node), along $c$, and thus we need  $\\dfrac{\\partial d}{\\partial c} = -\\dfrac{1}{c^{2}}$\n",
    "\n",
    "C) We need to step back once more from $c$ to $y$, and thus we need $\\dfrac{\\partial c}{\\partial y} = 1$\n",
    "\n",
    "D) To get the final derivative, we simply take the product $\\dfrac{\\partial d}{\\partial c}\\dfrac{\\partial c}{\\partial y} = \\dfrac{\\partial d}{\\partial y} = -\\dfrac{1}{c^{2}}$\n",
    "\n",
    "Again, $x=0, y=1$, $c=2$, and therefore  $\\dfrac{\\partial f}{\\partial y} = \\dfrac{\\partial d}{\\partial y} = -\\frac{1}{4}$.\n",
    "\n",
    "How would we hardcode this? \n",
    "\n",
    "    def dfdy(x,y):\n",
    "        a = -x\n",
    "        b = math.e ** a\n",
    "        c = y + b\n",
    "        d = 1 /c\n",
    "        return d\n",
    "        \n",
    "        dcdy = 1\n",
    "        dddc = -1/c**2\n",
    "        dddy = np.prod([dcdy,dddc])\n",
    "        \n",
    "### Review:\n",
    "\n",
    "Would this work with vectors of numbers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Class Exercise 1\n",
    "\n",
    "Compute the partial derivative $\\dfrac{\\partial f}{\\partial x}$ on $f(0,1)$ by hand using the pathway method shown in the previous section.\n",
    "\n",
    "![full-length-graph](./images/full_path_1.PNG)\n",
    "\n",
    "\n",
    "Solution:\n",
    "\n",
    "    def dfdx(x,y):\n",
    "        a = -x\n",
    "        b = math.e ** a\n",
    "        c = y + b\n",
    "        d = 1 /c\n",
    "        return d\n",
    "        \n",
    "        \n",
    "        dadx = -1\n",
    "        dbdx = math.e ** a\n",
    "        dcdb = 1\n",
    "        dddc = -1/c**2\n",
    "        dddx = np.prod([dadx, dbda, dcdb, dddc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Graphs to Autodifferentiation\n",
    "\n",
    "The idea behind a computational graph is fundamentally simple: A computation of any kind can be represented as a a stateful process, with operations represented as nodes and values represented at edges. The calculation proceeds along edges, combining the scalar values of the previous two edges, and using the operation marked at the node. This result is then passed along to the next edge.\n",
    "\n",
    "This idea is best illustrated with a simple example.\n",
    "\n",
    "Consider the formula:\n",
    "\n",
    "$e = (a+b)(b+1)$\n",
    "\n",
    "There are three operations.\n",
    "\n",
    "Two additions: $c = a+b$ and $d = b+1$\n",
    "\n",
    "and\n",
    "\n",
    "One multiplication: $e = cd$\n",
    "\n",
    "We can construct this operation in terms of a single three-node graph. The [source](http://colah.github.io/posts/2015-08-Backprop/) material references in the problem we explore here draws this diagram as follows:\n",
    "\n",
    "![tree-def1](./images/tree-def_1-1.png)\n",
    "\n",
    "(There is no one official way to draw these diagrams, but top-down is the most common.) Note that the author draws both the operations and the scalar value of the variables at the nodes (why? Think from the perspective of a programmer.). Another equally valid way to draw the same graph might be left to right so that the \"top\" is the rightmost node:\n",
    "\n",
    "![graphic-def1](./images/alternative_graphic_1-1.png)\n",
    "\n",
    "Note how much the shape of the image and the choice of notation seems to change the meaning of the data. Also note that the second graph does not appear to indicate an input layer. However the two graphs are fundamentally equivalent. We will be looking at this form for the remainder of this section.\n",
    "\n",
    "Following along with the source, we might envision the entire graph as a single function (it is) and use it as a formula for computation. For example, we might set $a=2$ and $b=1$, and see that the expression evaluates to $e=6$. We might further set $a=3$ and $b=6$ and thus see that the graph evaluates to $e=126$. \n",
    "\n",
    "## Review\n",
    "1. Please do these computations yourself. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives on Graphs\n",
    "\n",
    "\n",
    "### Forward Derivative\n",
    "\n",
    "![forward-derivative](./images/forward_derivative_1-1.png)\n",
    "\n",
    "\n",
    "So now we might turn our attention to the question of getting desired derivatives from our expression. First take the derivative in the traditional sense, using the sum and product rule:\n",
    "\n",
    "$$c = a+b$$\n",
    "\n",
    "$$d = b+1$$\n",
    "\n",
    "$$e = cd$$\n",
    "\n",
    "$$\\dfrac{\\partial c}{\\partial a} = 1$$\n",
    "\n",
    "$$\\dfrac{\\partial c}{\\partial b} = 1$$\n",
    "\n",
    "$$\\dfrac{\\partial d}{\\partial b} = 1$$\n",
    "\n",
    "$$\\dfrac{\\partial e}{\\partial d} = c\\dfrac{\\partial d}{\\partial b} = (a+b) = 3$$\n",
    "\n",
    "$$\\dfrac{\\partial e}{\\partial c} = d\\dfrac{\\partial c}{\\partial b} = (b+1) = 2$$\n",
    "\n",
    "(using $a=2$, $b=1$ in this case)\n",
    "\n",
    "The graph also offers convenient ways to think about this. First, as in the source, we will turn our attention to the **forward derivitave**. This might be thought of as the \"bottom-up\" (in the source's case) or \"left-to-right\" (in our case) scenario, taking derivatives of each variable dependent on the previous node. This means that for each node of variable $X$, we have to apply $\\dfrac{\\partial }{\\partial X}$ to all children of the subsequent node. In so doing, we accumulate the derivatives of each inner subexpression recursively:\n",
    "\n",
    "$$\\dfrac{\\partial y}{\\partial x} = \\dfrac{\\partial y}{\\partial w_{1}}\\dfrac{\\partial w_{1}}{\\partial x} =  \\dfrac{\\partial y}{\\partial w_{1}}\\left(\\dfrac{\\partial w_{1}}{\\partial w_{2}}\\dfrac{\\partial w_{2}}{\\partial x}\\right) \\cdots$$\n",
    "\n",
    "This process ends when we arrive at a final dependency of our target variable inside the expression.\n",
    "\n",
    "### Reverse Derivative\n",
    "\n",
    "This of course is very nice, but suppose what might happen if we wished to make this computation on a very large graph (production-scale networks often include millions of parameters). You would be forced to recompute the same derivatives many times over as the calculation went over every branch of the tree. This is particularly nonideal in the case of NN, because we are only really interested in one derivative, $\\dfrac{\\partial J}{\\partial W}$. \n",
    "\n",
    "In this case, we can apply the **reverse derivative**, which if we are seeking the derivative of node $Z$, then we can apply $\\dfrac{\\partial Z}{\\partial }$ (called the *adjoint*) to every previous node, extending back from $Z$ (which in the case of a NN, would be $J$). When we take the reverse derivative, we **start** with the derivative of our final goal with respect to itself (trivial as this is always 1), and then accumulate the **outer** subexpressions, progressing \"right-to-left\" on the graph. \n",
    "\n",
    "![reverse-derivative](./images/reverse_derivative_1-1.png)\n",
    "\n",
    "\n",
    "This is the same as following the below equation:\n",
    "\n",
    "$$\\dfrac{\\partial y}{\\partial x} = \\dfrac{\\partial y}{\\partial w_{1}}\\dfrac{\\partial w_{1}}{\\partial x} =  \\left(\\dfrac{\\partial y}{\\partial w_{2}}\\dfrac{\\partial w_{2}}{\\partial w_{1}}\\right)\\dfrac{\\partial w_{1}}{\\partial x} \\cdots$$\n",
    "\n",
    "In order to do so, we construct the derivatives of $e$ with respect to each subexpression. Remember, we stored them:\n",
    "\n",
    "$$c = a+b$$\n",
    "\n",
    "$$d = b+1$$\n",
    "\n",
    "$$e = cd$$\n",
    "\n",
    "$$\\dfrac{\\partial e}{\\partial e} = 1$$\n",
    "\n",
    "$$\\dfrac{\\partial c}{\\partial a} = 1 $$\n",
    "\n",
    "$$\\dfrac{\\partial c}{\\partial b} = 1 $$\n",
    "\n",
    "$$\\dfrac{\\partial d}{\\partial a} = 0 $$\n",
    "\n",
    "$$\\dfrac{\\partial d}{\\partial b} = 1 $$\n",
    "\n",
    "$$\\dfrac{\\partial e}{\\partial c} = d = (b+1) = 2$$\n",
    "\n",
    "$$\\dfrac{\\partial e}{\\partial d} = c = (a+b) = 3$$\n",
    "\n",
    "$$\\dfrac{\\partial e}{\\partial a} =\n",
    "\\left(c\\dfrac{\\partial d}{\\partial a} + d\\dfrac{\\partial c}{\\partial a}\\right) =(b+1) = 2 $$\n",
    "\n",
    "$$\\dfrac{\\partial e}{\\partial b} = \\left(c\\dfrac{\\partial d}{\\partial b} + d\\dfrac{\\partial c}{\\partial b}\\right) = (a+b) + (b+1) = 5$$\n",
    "\n",
    "Notice that we are accumulating previously calculated expressions as we proceed down the tree. Looking back at the tree diagram, we can see that the final expressions are exactly equivalent to operating on the value at each node based on the known derivative values of its parents.\n",
    "\n",
    "In fact, we can take the numerical derivative (the numbers), by simply computing them in the same way, multiplying as we go down which is really rather the point. This turns a previously difficult programming problem into one which we should be already quite familiar with...\n",
    "\n",
    "#### As an optimization\n",
    "\n",
    "For any given function $f(x_{1}, x_{2}, \\ldots, x_{n})$ described by a computational graph with $m$ nodes, running the full forward derivative works in $O(nm)$ time and $O(m)$ space. Backpropagation is a **powerful optimization** wherein derivatives take $O(m)$ time and $O(1)$ space. \n",
    "\n",
    "Mathematically, if the chain rule states:\n",
    "\n",
    "$$\\dfrac{\\partial f}{\\partial v} = \\sum_{i}\\dfrac{\\partial f}{\\partial u_{i}} \\dfrac{\\partial u_{i}}{\\partial v}$$\n",
    "\n",
    "\n",
    "If we use a computational graph to express a final node $z$ and input nodes $a_{i}$ and a single path from $z$ to $a_{i}$ goes through a set of intermediate nodes $a_{p}$ the derivative $\\dfrac{\\partial z}{\\partial a_{i}}$ can be expressed as follows:\n",
    "\n",
    "$$\\dfrac{\\partial z}{\\partial a_{i}} = \\sum_{p \\in parents(i)}\\dfrac{\\partial z}{\\partial a_{p}} \\dfrac{\\partial a_{p}}{\\partial a_{i}}$$\n",
    "\n",
    "\n",
    "\n",
    "### Review:\n",
    "1. What type of programming problem is this?\n",
    "2. Do the algorithmic analysis to convince yourself of the complexity figures I just gave you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing Autodifferentiation\n",
    "\n",
    "The above two exercises likely seem laborious in part due to the excessive notation obscuring obvious relationships. For comfort's sake, let's introduce the following notation; we will use the familiar $\\nabla$ (used for grad operation), but let's  restrict it to one variable only:\n",
    "\n",
    "$$\\nabla_{x} = \\dfrac{\\partial }{\\partial x}$$\n",
    "\n",
    "(Say \"grad of x\")\n",
    "\n",
    "Now we can decompose our first problem down in terms of these new partial gradients as illustrated below:\n",
    "\n",
    "$$\\nabla_{d} = \\dfrac{\\partial d}{\\partial d}$$\n",
    "\n",
    "$$\\nabla_{c} = \\dfrac{\\partial d}{\\partial c}\\nabla_{d}$$\n",
    "\n",
    "$$\\nabla_{y} = \\dfrac{\\partial c}{\\partial y}\\nabla_{c}$$\n",
    "\n",
    "$$\\nabla_{b} = \\dfrac{\\partial c}{\\partial b}\\nabla_{c}$$\n",
    "\n",
    "$$\\nabla_{a} = \\dfrac{\\partial b}{\\partial a}\\nabla_{b}$$\n",
    "\n",
    "$$\\nabla_{x} = \\dfrac{\\partial a}{\\partial x}\\nabla_{a}$$\n",
    "\n",
    "![grads_intro](./images/grad_introduction.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Class Exercise 2\n",
    "\n",
    "Given the formula $f(x, y, z) = \\left[max(wx, 0)-y\\right]^{2}+z$ consider $f(1, 0, -2)$, $w=3$. Compute the targets $\\nabla_{x}$, $\\nabla_{y}$, $\\nabla_{z}$ using the above methodology. Write out the whole graph for yourself.\n",
    "\n",
    "![partial-grads-exercise](./images/exercise2_1.PNG)\n",
    "\n",
    "Solution:\n",
    "\n",
    "    def df(x, y, z, w=3):\n",
    "        p = x * w\n",
    "        q = max(p, 0)\n",
    "        r = q - y\n",
    "        s = r ** 2\n",
    "        t = s + z\n",
    "        \n",
    "        dt = 1\n",
    "        ds, dz = 1*dt, 1*dt\n",
    "        dr = 2*r*ds\n",
    "        dq, dy = 1*dr, -1*dr\n",
    "        dp = (p>0)*dq\n",
    "        dx, dw = w*dp, x*dp\n",
    "        \n",
    "        return dx, dw, dy, dz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving from Scalars to Vectors\n",
    "\n",
    "If we generalize the function featured in exercise 2 above to include all parameters, we might be looking at something that reminds us a bit more of a NN:\n",
    "\n",
    "$$f(x, y, z) = \\left[max(wx, 0)-y\\right]^{2}+z$$\n",
    "\n",
    "Suppose we step from this scalar formality into **vectorizing it**? Of course this is the ideal situation for NN, because we have, say, $N$ training examples, each of dimension $H$. Computing all this in series is an unnecessary computational expense. We wish to abstract away the paralell parts of the operation. \n",
    "\n",
    "We need to formulate the function in terms of vector-matrix notation:\n",
    "\n",
    "$\\bf{p} = \\bf{x}W$ ($(1 \\times N) \\cdot (N \\times H)$ to produce a $1 \\times H$) input vector - we are weighting the value of the input<br/><br/>\n",
    "\n",
    "$f(x,y,z) = \\left[max(\\bf{p},0)-\\bf{y}\\right]^{2}+\\bf{z}$\n",
    "\n",
    "![vectorizing](./images/vectorization_1.PNG)\n",
    "\n",
    "\n",
    "As can be seen in the above figure, the linear operations map almost exactly to vector operations as follows:\n",
    "\n",
    "$\\nabla_{t} = \\bf{1} $ (this is a vector of 1 values) <br/><br/>\n",
    "$\\nabla_{s} = \\nabla_{z} = \\bf{1} \\otimes \\nabla_{t}$ (simple multiplications generalize to outer products) <br/><br/>\n",
    "$\\nabla_{r} = 2\\bf{r} \\otimes \\nabla_{s}$ <br/><br/>\n",
    "$\\nabla_{y} = -\\bf{1} \\otimes \\nabla_{r}$ <br/><br/>\n",
    "$\\nabla_{q} = \\bf{1} \\otimes \\nabla_{r}$ <br/><br/>\n",
    "$\\nabla_{p} = \\left\\{\\bf{p} \\gt 0 \\right\\} \\otimes \\nabla_{q}$ <br/><br/>\n",
    "$\\nabla_{w} = \\bf{x^{T}}\\nabla_{p}$ ($(N \\times 1) \\cdot (1 \\times H) = N \\times H$ inner product, to match dimensions of $W$) - matrix products must obey dimensionality <br/><br/>\n",
    "$\\nabla_{x} = \\nabla_{p}\\bf{W^{T}} $ ($(1 \\times H) \\cdot (H \\times N) = 1 \\times N$ inner product, to match dimensions of $\\bf{x}$) <br/><br/>\n",
    "\n",
    "#### A vectorized version of the above code:\n",
    "\n",
    "\n",
    "    def df(x, y, z, w):\n",
    "        p = x @ w # @ is an alias for np.matmul() - the full vector inner product - check dimensions!\n",
    "        q = np.max(p, 0)\n",
    "        r = q - y\n",
    "        s = r ** 2\n",
    "        t = s + z\n",
    "        \n",
    "        dt = np.ones_like(t)\n",
    "        ds, dz = 1*dt, 1*dt\n",
    "        dr = 2*r*ds\n",
    "        dq, dy = 1*dr, -1*dr\n",
    "        dp = (p>0)*dq\n",
    "        dx, dw = dp@W.T, x.T@dp\n",
    "        \n",
    "        return dx, dw, dy, dz\n",
    "\n",
    "\n",
    "\n",
    "### Moving from Vectors to Batches\n",
    "\n",
    "Moving from vectors to batches of size $B$ is quite easy, because of the setups and geometry we've already chosen. Now we need only include $B$ rows of the input vectors $x$ of size $1 \\times N$ and process vectors $(\\bf{p,q, r, s})$ of size $1 \\times H$. Now $x$ becomes $X$, a $B \\times N$ block matrix, and $(\\bf{p,q, r, s})$ become $(\\bf{P,Q,R,S})$, a $B \\times H$ block matrices. Everything else stays the same.\n",
    "\n",
    "![batching](./images/batching_1.PNG)\n",
    "\n",
    "\n",
    "#### A minibatched version of the above code:\n",
    "\n",
    "    def df(X, y, z, W):\n",
    "        P = X @ W # @ is an alias for np.matmul() - the full vector inner product\n",
    "        Q = np.max(P, 0)\n",
    "        R = Q - y\n",
    "        S = R ** 2\n",
    "        T = S + z\n",
    "        \n",
    "        dT = np.ones_like(T)\n",
    "        dS, dz = 1*dT, dT.sum(axis=0)\n",
    "        dR = 2*R*dS\n",
    "        dQ, dy = 1*dr, -1*dR.sum(axis=0)\n",
    "        dP = (P>0)*dQ\n",
    "        dX, dW = dP@W.T, X.T@dP\n",
    "        \n",
    "        return dX, dW, dy, dz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Class Exercise 3: A Last Word about Multiple Gradients:\n",
    "\n",
    "As intimated above, an expression in a computation graph where one node goes to two (or more) inputs is in fact quite likely. In fact, we've already covered it above, but just to drive the point home, we will consider it again. These are the cases where autodifferentiation is really desirable. Consider this problem:\n",
    "\n",
    "Compute $\\nabla_{x}$ for $w = f(x) = x^{2} + 3x$\n",
    "\n",
    "![multi-gradients](./images/multi_gradients.PNG)\n",
    "\n",
    "\n",
    "#### Solution:\n",
    "\n",
    "$$\\nabla_{w} = 1$$\n",
    "\n",
    "$$\\nabla_{y} = \\dfrac{\\partial w}{\\partial y}\\nabla_{w} = 1$$\n",
    "\n",
    "$$\\nabla_{x, 1} = \\dfrac{\\partial y}{\\partial x}\\nabla_{y} = 2x$$\n",
    "\n",
    "$$\\nabla_{z} = \\dfrac{\\partial w}{\\partial z}\\nabla_{w} = 1$$\n",
    "\n",
    "$$\\nabla_{x, 2} = \\dfrac{\\partial z}{\\partial x}\\nabla_{z} = 3$$\n",
    "\n",
    "$$\\nabla_{x, 0} = \\nabla_{x, 1} + \\nabla_{x, 2} = 2x + 3$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
