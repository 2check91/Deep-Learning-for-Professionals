{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A quick introduction to NLP and Embeddings with Deep Learning\n",
    "\n",
    "Do you think embeddings would help [here?](https://www.youtube.com/watch?v=SacogDL_4JU)\n",
    "\n",
    "With help and excerpts [from Chris Olah](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/).\n",
    "\n",
    "<center><img src=\"http://rutumulkar.com/public/images/blog/nlp-ml.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Natural language processing** (NLP) is a subfield co-disciplinary between linguistics (where it is often called \"computational linguistics\"), statistics and applied computer science. It does not share overly much in common with the algorithmic or deeply theoretical component of computer science.\n",
    "\n",
    "NLP practitioners focus on methods of allowing humans to interact meaningfully with computer using different types of natural language. Sometimes this means that a human speaks verbally to the computer. In other cases the human writes words down or enters them via a keyboard. All three of the above cases has historically required a separate set of tools and approaches.\n",
    "\n",
    "The [vast body of work](https://en.wikipedia.org/wiki/Natural_language_processing) pertaining to NLP is the subject matter of one or more semester-long classes and is beyond the purview of this course. However the role of deep learning in NLP is a growing - debatably critical - one, and so we must at least treat it minimally here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Review (In class)\n",
    "Read the provided background [review]('../readings/amiajnl-2011-000464.pdf') and skim in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings - The Future\n",
    "\n",
    "The great power of Google to search a phrase by its context and subject seems to come from an almost implicit understanding of the **meanings** of the words within the phrase. How does this happen? Word embeddings - Word embeddings emerged from the time-series discipline of temporal embedding. Temporal embedding comes from the studies of chaotics developed by [Takens](https://en.wikipedia.org/wiki/Takens%27_theorem) and colleagues during the 1960-2010 period of modern mathematics. Example papers [here](http://www.math.colostate.edu/~shipman/47/volume12009/rohrbacker.pdf) and here show the simplicity and power of this [approach](https://www.pks.mpg.de/~tisean/Tisean_3.0.0/docs/chaospaper/node6.html)\n",
    "\n",
    "There's an example of a mix between temporal embeddings and neural nets [here](https://arxiv.org/pdf/1709.01073.pdf) - obvious, really, but nicely done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Learn Word Embeddings?\n",
    "(excerpted from [here](https://www.tensorflow.org/tutorials/word2vec#the_skip-gram_model)\n",
    "\n",
    "Image and audio processing systems work with rich, high-dimensional datasets encoded as vectors of the individual raw pixel-intensities for image data, or e.g. power spectral density coefficients for audio data. For tasks like object or speech recognition we know that all the information required to successfully perform the task is encoded in the data (because humans can perform these tasks from the raw data). \n",
    "\n",
    "However, natural language processing systems traditionally treat words as discrete atomic symbols, and therefore 'cat' may be represented as 'Id537' and 'dog' as 'Id143'. These encodings are arbitrary, and provide no useful information to the system regarding the relationships that may exist between the individual symbols.\n",
    "\n",
    "This means that the model can leverage very little of what it has learned about 'cats' when it is processing data about 'dogs'. Consider the similarities between the two that could be gleaned from photos:\n",
    "\n",
    "Mammals\n",
    "Four-Legged\n",
    "Furry\n",
    "Pets\n",
    "Trainable\n",
    "Wear Collars\n",
    "Sleep on your bed\n",
    "\n",
    "\n",
    "Representing words as unique, discrete ids furthermore leads to data sparsity, and usually means that we may need more data in order to successfully train statistical models. Using vector representations can overcome some of these obstacles.\n",
    "\n",
    "   ![images](./images/audio-image-text.png)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "Word embeddings are one of the best places to gain intuition about why deep learning is so effective.\n",
    "\n",
    "A word embedding $W: words \\rightarrow \\mathbb{R}^n$\n",
    "\n",
    "is a paramaterized function mapping words in some language to high-dimensional vectors (perhaps 200 to 500 dimensions). For example, we might find:\n",
    "\n",
    "$$W(\"cat\")=(0.2, -0.4, 0.7, ...)$$\n",
    "\n",
    "$$W(\"mat\")=(0.0, 0.6, -0.1, ...)$$\n",
    "\n",
    "(Typically, the function is a lookup table, parameterized by a matrix, $\\theta$, with a row for each word: \n",
    "$W_{\\theta}(w_{n}) = \\theta_{n}.)\n",
    "\n",
    "The standard process is to initialize W to start with completely random vectors for each word. The embedding is then optimized to have meaningful vectors for each word. For example, we might train a network for predicting whether or not a 5-gram (sequence of five words) is ‘valid.’ (logical)\n",
    "\n",
    "We can easily get lots of 'valid' 5-grams from Wikipedia (eg. “cat sat on the mat”) and then ‘break’ half of them by switching a word with a random word (eg. “cat sat phonograph the mat”), since that will almost certainly make our 5-gram nonsensical.\n",
    "\n",
    "Now we build a typical DL model to intake these 5-grams and label them valid or invalid:\n",
    "\n",
    "[5-gram-model]()\n",
    "\n",
    "The model we train will run each word in the 5-gram through W to get a vector representing it and feed those into another ‘module’ called R\n",
    "\n",
    "which tries to predict if the 5-gram is ‘valid’ or ‘broken.’ Then, we’d like:\n",
    "\n",
    "$$R(W(\"cat\"), W(\"sat\"), W(\"on\"), W(\"the\"), W(\"mat\"))=1$$\n",
    "\n",
    "$$R(W(\"cat\"), W(\"sat\"), W(\"phonograph\"), W(\"the\"), W(\"mat\"))=0$$\n",
    "\n",
    "In order to predict these values accurately, the network needs to learn good parameters for both $W$ and $R$.\n",
    "\n",
    "This is not a great model for any particular use, but point of the task is to learn W.Many other tasks could have been used to learn $W$ – another common one is predicting the next word in the sentence. But we don’t really care. The method of training the word produces the embedding $W$, which is the only desired end. \n",
    "\n",
    "What is the embedding like? One thing we can do to get a feel for the word embedding space is to visualize them with t-SNE, a sophisticated technique for visualizing high-dimensional data:\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<center><img src=\"./images/Turian-WordTSNE.png\" width=\"600\"/></center>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "This map of relationships between words makes a lot of sense to a human. It means that similar words are \"physically\" \"close\" to one another. We can visualize this in another way:\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<center><img src=\"./images/Colbert-WordTable2.png\" width=\"500\"/></center>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "How does this happen? It seems quite likely that there are lots of situations where it has seen a sentence like “the wall is blue” and know that it is valid before it sees a sentence like “the wall is red”. As such, shifting the “red” vector a bit closer to “blue” makes the network perform better. Imagine this repeated across a vast sample.\n",
    "\n",
    "Word embeddings exhibit an even more remarkable property: analogies between words seem to be encoded in the difference vectors between words. For example, there seems to be a constant male-female difference vector:\n",
    "\n",
    "<br/>\n",
    "<center><img src=\"./images/Mikolov-GenderVecs.png\" width=\"200\"/></center>\n",
    "<br/>\n",
    "\n",
    "$$W(\"woman\")-W(\"man\") \\simeq W(\"aunt\")-W(\"uncle\")$$\n",
    "$$W(\"woman\")-W(\"man\") \\simeq W(\"queen\")-W(\"king\")$$\n",
    "\n",
    "<br/>\n",
    "<center><img src=\"./images/Mikolov-AnalogyTable.png\" width=\"400\"/></center>\n",
    "<br/>\n",
    "\n",
    "\n",
    "It’s important to appreciate that all of these properties of $W$ are side effects - an accident. We didn’t try to have similar words be close together. We didn’t try to have analogies encoded with difference vectors. All we tried to do was perform a simple task, like predicting whether a sentence was valid. \n",
    "\n",
    "This seems to be a great strength of neural networks: they learn better ways to represent data, automatically. Representing data well is an essential part of applying machine learning successfully. Word embeddings are just a particularly striking example of learning a representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Space Models\n",
    "\n",
    "Vector space models (VSMs) represent (embed) words in a continuous vector space where semantically similar words are mapped to nearby points ('are embedded nearby each other'). VSMs have a long, rich history in NLP, but all methods depend in some way or another on the [Distributional Hypothesis](http://soda.swedish-ict.se/3941/1/sahlgren.distr-hypo.pdf), which states that words that appear in the same contexts share semantic meaning. \n",
    "\n",
    "The different approaches that leverage this principle can be divided into two categories: count-based methods (e.g. Latent Semantic Analysis), and predictive methods (e.g. neural probabilistic language models).\n",
    "\n",
    "This issue has been discussed in detail by [Baroni et al.](http://clic.cimec.unitn.it/marco/publications/acl2014/baroni-etal-countpredict-acl2014.pdf). In a nutshell: Count-based methods compute the statistics of how often some word co-occurs with its neighbor words in a large text corpus, and then map these count-statistics down to a small, dense vector for each word. Predictive models directly try to predict a word from its neighbors in terms of learned small, dense embedding vectors (considered parameters of the model).\n",
    "\n",
    "Word2vec comes in two flavors: the Continuous Bag-of-Words model (CBOW) and the Skip-Gram model (Section 3.1 and 3.2 in [Mikolov et al.](https://arxiv.org/pdf/1301.3781.pdf)). Algorithmically, these models are similar, except that CBOW predicts target words (e.g. 'mat') from source context words ('the cat sits on the'), while the skip-gram does the inverse and predicts source context-words from the target words. \n",
    "\n",
    "The smoothing created treating an entire context as one observation removes a great deal of distributional information and so CBOW does better in smaller datasets where sampling of the parameter space is likely to be less complete. Skip-Gram, conversely, does better in large datasets where the sampling is much more likely to be completely converged and there are more target-context pairs.\n",
    "\n",
    "## What they're good for\n",
    "\n",
    "Word2Vec (discussed below) is a typical parameterization used in NLP tasks. Now, instead of fighting the machine over a reasonable representation of the dataset, a researcher can simply unpack the Word2Vec embedding and go on using it as input to a model. That's the big leap forward provided by embedding. No embeddings, however are perfect, and so there's always room for improvement - research is ongoing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec: Skip-gram Model\n",
    "\n",
    "As an example, let's consider the dataset\n",
    "\n",
    "    the quick brown fox jumped over the lazy dog\n",
    "\n",
    "We must first define the words in context to build the dataset (context being a feature) - there are many ways to do so. For now, let's stick to the vanilla definition and define 'context' as the window of words to the left and to the right of a target word. Using a window size of 1, we then have the dataset:\n",
    "\n",
    "    ([the, brown], quick), ([quick, fox], brown), ([brown, jumped], fox), ...\n",
    "\n",
    "of (context, target) pairs. Recall that skip-gram inverts contexts and targets, and tries to predict each context word from its target word, so the task becomes to predict 'the' and 'brown' from 'quick', 'quick' and 'fox' from 'brown', etc. Therefore our dataset becomes\n",
    "\n",
    "    (quick, the), (quick, brown), (brown, quick), (brown, fox), ...\n",
    "\n",
    "of (input, output) pairs. The objective function is defined over the entire dataset, but we typically optimize this with stochastic gradient descent (SGD) using one example at a time (or a 'minibatch' of batch_size examples, where typically 16 <= batch_size <= 512).\n",
    "\n",
    "Let's examine a single step of the optimization process using the first example.\n",
    "\n",
    "1. We select num_noise number of noisy (contrastive) examples by drawing from some noise distribution, typically the [unigram](http://qwone.com/~jason/writing/unigram.pdf) distribution. For simplicity let's say num_noise=1 and we select sheep as a noisy example.\n",
    "\n",
    "2. Next we compute the loss for this pair of observed and noisy examples, i.e. the objective at time step $t$ becomes\n",
    "\n",
    "$$J_{NEG}^{t} = log Q_{\\theta}(D=1|the, quick) + log( Q_{\\theta}(D=0|sheep, quick))$$\n",
    "\n",
    "The goal is to make an update to the embedding parameters $\\theta$ to improve (in this case, maximize) this objective function, taking the gradient of the loss with respect to the embedding parameters $\\theta$, i.e. $\\dfrac{\\partial}{\\partial \\theta}J_{NEG}$\n",
    "\n",
    "We then perform an update to the embeddings by taking a small step in the direction of the gradient. When this process is repeated over the entire training set, this has the effect of 'moving' the embedding vectors around for each word until the model is successful at discriminating real words from noise words.\n",
    "\n",
    "We can visualize the learned vectors by projecting them down to 2 dimensions using for instance something like the [t-SNE dimensionality reduction technique](http://lvdmaaten.github.io/tsne/). When we inspect these visualizations it becomes apparent that the vectors capture some general, and in fact quite useful, semantic information about words and their relationships to one another. This is visualized below. (see [here](http://www.aclweb.org/anthology/N13-1090) also).\n",
    "\n",
    "<br/>\n",
    "<center><img src=\"./images/linear-relationships.png\" width=\"600\"/></center>\n",
    "<br/>\n",
    "\n",
    "\n",
    "The context of the word is the key measure of meaning that is utilized in Word2Vec.  The context of the word “sat” in the sentence “the cat sat on the mat” is (“the”, “cat”, “on”, “the”, “mat”).  In other words, it is the words which commonly occur around the target word “sat”. Words which have similar contexts share meaning under Word2Vec, and their reduced vector representations will be similar.  In the skip-gram model version of Word2Vec (more on this later), the goal is to take a target word i.e. “sat” and predict the surrounding context words.  This involves an iterative learning process.\n",
    "\n",
    "The end product of this learning will be an embedding layer in a network – this embedding layer is a kind of lookup table – the rows are vector representations of each word in our vocabulary.  Here’s a simplified example (using dummy values and one-hot vectors) of what this looks like, where vocabulary_size=7 and embedding_size=3:\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|c c c}\n",
    "anarchism &amp; 0.5 &amp; 0.1 &amp; -0.1\\\\\n",
    "originated &amp; -0.5 &amp; 0.3 &amp; 0.9 \\\\\n",
    "as &amp; 0.3 &amp; -0.5 &amp; -0.3 \\\\\n",
    "a &amp; 0.7 &amp; 0.2 &amp; -0.3\\\\\n",
    "term &amp; 0.8 &amp; 0.1 &amp; -0.1 \\\\\n",
    "of &amp; 0.4 &amp; -0.6 &amp; -0.1 \\\\\n",
    "abuse &amp; 0.7 &amp; 0.1 &amp; -0.4\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "As you can see, each word (row) is represented by a vector of size 3.  Learning this embedding layer/lookup table can be performed using a simple neural network and an output softmax layer – see the diagram below:\n",
    "\n",
    "<br/>\n",
    "<center><img src=\"./images/Word2Vec-softmax.jpg\" width=\"500\"/></center>\n",
    "<br/>\n",
    "\n",
    "The idea of the neural network above is to supply our input target words as one-hot vectors. Then, via a hidden layer, we want to train the neural network to increase the probability of valid context words, while decreasing the probability of invalid context words (i.e. words that never show up in the surrounding context of the target words).  This involves using a softmax function on the output layer. Once training is complete, the output layer is discarded, and our embedding vectors are the weights of the hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Implementing Word2Vec in Keras\n",
    "\n",
    "Using the [adventuresinmachinelearning](http://adventuresinmachinelearning.com/word2vec-keras-tutorial/) source\n",
    "\n",
    "In this implementation we will be considering the skip-gram variant (for more details – see this tutorial). Using a default implementation of deep learning can have disastrous results - a naive, softmax-based word embedding training regime results in an extremely slow training of the embedding layer when using large training vocabularies.  To get around this problem, a technique called “negative sampling” has been proposed, and a custom loss function has been created in TensorFlow to allow this (nce_loss).\n",
    "\n",
    "Unfortunately, this loss function doesn’t exist in Keras so we are going to implement it ourselves.  This is a fortunate omission, as implementing it ourselves will help us to understand how negative sampling works and therefore better understand the Word2Vec Keras process.\n",
    "\n",
    "#### The softmax issue and negative sampling\n",
    "\n",
    "The problem with using a full softmax output layer is that it is very computationally expensive.  Consider the definition of the softmax function:\n",
    "\n",
    "$$P(y=j∣x)=\\dfrac{e^{x^{^Tw_{j}}}}{\\sum_{k=1}^{K}e^{x^{^Tw_{k}}}}$$\n",
    "\n",
    "Here the probability of the output being class $j$ is calculated by multiplying the output of the hidden layer and the weights connecting to the class $j$ output on the numerator and dividing it by the same product but over all the remaining weights.  When the output is a 10,000-word one-hot vector, there are tens of millions of weights that need to be updated in the gradient at each cycle, a prohibitive barrier to optimization.\n",
    "\n",
    "There’s another solution called negative sampling, described in the original Word2Vec paper.  It works by reinforcing the strength of weights which link a target word to its context words, but rather than reducing the value of **all** those weights which aren’t in the context, it simply updates \"samples\" a small number of them – these are called the “negative samples”.\n",
    "\n",
    "To train the embedding layer using negative samples in Keras (rather than Tensorflow), we re-imagine the way we train our network.  Instead of constructing our network so that the output layer is a multi-class softmax layer, we can change it into a simple binary classifier.  For words that are in the context of the target word, we want our network to output a 1, and for our negative samples, we want our network to output a 0. Therefore, the output layer of our Word2Vec Keras network is simply a single node with a sigmoid activation function.\n",
    "\n",
    "To ensure that similar words have similar embedding vectors, we want to ensure that the trained network will always output a 1 when it is supplied words which are in the same context, but 0 when it is supplied words which are never in the same context. To do this a vector similarity score is supplied to the output sigmoid layer – with similar vectors outputting a high score and dissimilar vectors outputting a low score. Here we use the common cosine similarity score:\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "\n",
    "$$cos(\\theta) = \\frac{\\textbf{A}\\cdot\\textbf{B}}{\\parallel\\textbf{A}\\parallel_2 \\parallel \\textbf{B} \\parallel_2}$$\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<p>The negative sampling network for the planned Word2Vec Keras implementation features:</p>\n",
    "<ul>\n",
    "<li>An (integer) input of a target word and a real or negative context word</li>\n",
    "<li>An embedding layer lookup (i.e. looking up the integer index of the word in the embedding matrix to get the word vector)</li>\n",
    "<li>The application of a dot product operation</li>\n",
    "<li>The output sigmoid layer</li>\n",
    "\n",
    "<br/>\n",
    "<center><img src=\"./images/Negative-sampling-architecture-1.jpg\" width=\"700\"/></center>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract the information, we will be using some text extraction functions. Basically, the function calls other functions which download the data, then a function that converts the text data into a string of integers – with each word in the vocabulary represented by a unique integer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading functions\n",
    "import os\n",
    "import urllib\n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "def maybe_download(filename, url, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urllib.request.urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "# Read the data into a list of strings.\n",
    "def read_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "url = 'http://mattmahoney.net/dc/'\n",
    "filename = maybe_download('text8.zip', url, 31344016)\n",
    "vocabulary = read_data(filename)\n",
    "print(vocabulary[:7])\n",
    "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse']\n",
    "vocab_size = 10000\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words=vocabulary,n_words=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to define some constants for the training and also create a validation set of words so we can check the learning progress of our word vectors: \n",
    "\n",
    "* Window_size is the window of words around the target word that context will be calculated from. \n",
    "* Vector_dim is the size of each of our word embedding vectors.\n",
    "* Embedding layer will be of size 10,000 x 300.  \n",
    "* number of epochs will be large, as embedding is slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_size = 3\n",
    "vector_dim = 300\n",
    "num_epochs = 1000000\n",
    "\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The skip-gram function in Keras\n",
    "\n",
    "The data set has to be sampled using skip-gram (a relative of n-gram parameterization methods). \n",
    "\n",
    "**Skip-gram Algorithm:**\n",
    "\n",
    "1. Scan through the data set picking target words\n",
    "2. Randomly select context words from within the window of words around the target word (i.e. if the target word is “on” from “the cat sat on the mat”, with a window size of 2 the words “cat”, “sat”, “the”, “mat” could all be randomly selected as valid context words).\n",
    "3. Select negative samples outside of the selected target word context.\n",
    "4. Label the word 1 or 0, depending on whether the supplied context word is a true context word or a negative sample.  \n",
    "\n",
    "Thankfully, Keras has a function (skipgrams) which does all that for us – consider the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "sampling_table = sequence.make_sampling_table(vocab_size)\n",
    "couples, labels = sequence.skipgrams(data, vocab_size, window_size=window_size, sampling_table=sampling_table)\n",
    "word_target, word_context = zip(*couples)\n",
    "word_target = np.array(word_target, dtype=\"int32\")\n",
    "word_context = np.array(word_context, dtype=\"int32\")\n",
    "print(couples[:10], labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sequence.skipgrams` returns the word couples in the form of (target, context) and also gives a matching label of 1 or 0 depending on whether context is a true context word or a negative sample. By default, it returns randomly shuffled couples and labels. The couples tuple is then split into separate word_target and word_context variables and make sure they are the right type. \n",
    "\n",
    "(This may not run on your computer, depending on the amount of memory you have available.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying Keras\n",
    "\n",
    "The first argument to this layer definition is the number of rows of our embedding layer – which is the size of our vocabulary (10,000).  The second is the size of each word’s embedding vector (the columns) – in this case, 300. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_target = Input((1,))\n",
    "input_context = Input((1,))\n",
    "\n",
    "embedding = Embedding(vocab_size, vector_dim, input_length=1, name='embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights for this layer are initialized automatically, but you can also specify an optional embeddings_initializer argument whereby you supply a Keras initializer object.  Next, as per our architecture, we need to look up an embedding vector (length = 300) for our target and context words, by supplying the embedding layer with the word’s unique integer value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = embedding(input_target)\n",
    "target = Reshape((vector_dim, 1))(target)\n",
    "context = embedding(input_context)\n",
    "context = Reshape((vector_dim, 1))(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup a cosine similarity operation which will be output in a secondary model\n",
    "similarity = merge([target, context], mode='cos', dot_axes=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to continue on with our primary model architecture, and the dot product as our measure of similarity which we are going to use in the primary flow of the negative sampling architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now perform the dot product operation to get a similarity measure\n",
    "dot_product = merge([target, context], mode='dot', dot_axes=1)\n",
    "dot_product = Reshape((1,))(dot_product)\n",
    "# add the sigmoid output layer\n",
    "output = Dense(1, activation='sigmoid')(dot_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The keras `merge` is applied to target and context word vectors, with the mode argument set to ‘dot’ to get the simple dot product.  \n",
    "2. Another reshape layer is applied to take the reshaped dot product value (a single data point/scalar) and apply it to a Keras Dense layer with a sigmoid activation function. \n",
    "\n",
    "This is the output of our Word2Vec Keras architecture. **Now you can see at this point it doesn't matter what the outputs are or even what we train on, only that we gain benefit from the training the embedding to make a better prediction.**\n",
    "\n",
    "Next, we need to gather everything into a Keras model and compile it, ready for training, along with a comparison model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the primary training model\n",
    "model = Model(input=[input_target, input_context], output=output)\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "# create a secondary validation model to run our similarity checks during training\n",
    "validation_model = Model(input=[input_target, input_context], output=similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The similarity callback\n",
    "\n",
    "We want to create a “callback” which we can use to figure out which words are closest in similarity to our validation examples, so we can monitor the training progress of our embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimilarityCallback:\n",
    "    def run_sim(self):\n",
    "        for i in range(valid_size):\n",
    "            valid_word = reverse_dictionary[valid_examples[i]]\n",
    "            top_k = 8  # number of nearest neighbors\n",
    "            sim = self._get_sim(valid_examples[i])\n",
    "            nearest = (-sim).argsort()[1:top_k + 1]\n",
    "            log_str = 'Nearest to %s:' % valid_word\n",
    "            for k in range(top_k):\n",
    "                close_word = reverse_dictionary[nearest[k]]\n",
    "                log_str = '%s %s,' % (log_str, close_word)\n",
    "            print(log_str)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_sim(valid_word_idx):\n",
    "        sim = np.zeros((vocab_size,))\n",
    "        in_arr1 = np.zeros((1,))\n",
    "        in_arr2 = np.zeros((1,))\n",
    "        for i in range(vocab_size):\n",
    "            in_arr1[0,] = valid_word_idx\n",
    "            in_arr2[0,] = i\n",
    "            out = validation_model.predict_on_batch([in_arr1, in_arr2])\n",
    "            sim[i] = out\n",
    "        return sim\n",
    "sim_cb = SimilarityCallback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class runs through all the valid_examples and gets the similarity score between the given validation word and all the other words in the vocabulary. It gets the similarity score by running _get_sim(), which features a loop which runs through each word in the vocabulary, and runs a predict_on_batch() operation on the validation model – looking up the embedding vectors for the two supplied words (the valid_example and the looped vocabulary example) and returning a result.  The main loop then sorts and creates a string to print out the top 8 words with the closest similarity to the validation example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this loop, we run through the total number of epochs.  First, we select a random index from our word_target, word_context and labels arrays and place the values in dummy numpy arrays.  Then we supply the input ([word_target, word_context]) and outputs (labels) to the primary model and run a train_on_batch() operation.  This returns the current loss evaluation, `loss`, of the model and prints it. Every 10,000 iterations the SimilarityCallback is run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arr_1 = np.zeros((1,))\n",
    "arr_2 = np.zeros((1,))\n",
    "arr_3 = np.zeros((1,))\n",
    "for cnt in range(epochs):\n",
    "    idx = np.random.randint(0, len(labels)-1)\n",
    "    arr_1[0,] = word_target[idx]\n",
    "    arr_2[0,] = word_context[idx]\n",
    "    arr_3[0,] = labels[idx]\n",
    "    loss = model.train_on_batch([arr_1, arr_2], arr_3)\n",
    "    if i % 100 == 0:\n",
    "        print(\"Iteration {}, loss={}\".format(cnt, loss))\n",
    "    if cnt % 10000 == 0:\n",
    "        sim_cb.run_sim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output\n",
    "\n",
    "Here are some of the word similarity outputs for the validation example word “eight” as we progress through the training iterations:\n",
    "\n",
    "`Iterations = 0`:\n",
    "\n",
    "Nearest to eight: `much, holocaust, representations, density, fire, senators, dirty, fc`\n",
    "\n",
    "`Iterations = 50,000:`\n",
    "\n",
    "Nearest to eight: `six, finest, championships, mathematical, floor, pg, smoke, recurring`\n",
    "\n",
    "`Iterations = 200,000:`\n",
    "\n",
    "Nearest to eight: `six, five, two, one, nine, seven, three, four`\n",
    "\n",
    "As can be observed, at the start of the training, all sorts of random words are associated with “six”.  However, as the training iterations increase, slowly other word numbers are associated with “six” until finally all of the closest 8 words are number words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
