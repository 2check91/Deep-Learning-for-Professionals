{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks\n",
    "\n",
    "#### “(GANs), and the variations that are now being proposed is the most interesting idea in the last 10 years in ML, in my opinion.”\n",
    "#### -Yann LeCun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explain the concept of GANs, let's use an analogy:\n",
    "\n",
    "Imagine you want to buy good watches. If you never buy them it’s very likely that you can’t distinguish brand watches from fake ones. You have to be experienced to not let yourself be fooled by the seller.\n",
    "\n",
    "As you start to label most of the watches as fake (after a number of mistakes of course), the seller will start to “generate” more compelling copies of the watches. This example demonstrates the behavior of generative adversarial networks: Discriminator (buyer) and Generator (seller).\n",
    "\n",
    "These two networks, Discriminator and Generator, are in a contest with each other. The Generator is forced to generate realistic samples and the Discriminator learns to distinguish generated samples from real data. By doing so the generative network both learns the features of the data and is capable of generating new examples thereof.\n",
    "\n",
    "\n",
    "<center><img src=\"./images/GAN_layout.jpg\" height=\"250\"/></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn the Generator’s distribution, $p_g$ over data $x$, the distribution on input noise variables $p_z(z)$ should be defined. Then $G(z, \\theta_{g})$ maps z from latent space $Z$ to data space and $D(x, \\theta_d)$ outputs a single scalar — probability that x came from the real data rather than $p_g$.\n",
    "\n",
    "The Discriminator is trained to maximize the probability of assigning the correct label to both examples of real data and generated samples. While the Generator is trained to minimize $log(1 — D(G(z)))$. In other words — to minimize the probability of the Discriminator’s correct answer.\n",
    "\n",
    "It is possible to consider such a training task as [minimax](https://en.wikipedia.org/wiki/Minimax) game with value function $V(G, D)$:\n",
    "\n",
    "$$\\underset{D}{min}\\ \\underset{G}{max}\\ V(D,G) = {\\mathbb{E}}_{x \\sim p_{data}}[log(D(x))]+ {\\mathbb{E}}_{z \\sim p_{z}(z)}[log(1-G(D(z)))]$$\n",
    "\n",
    "Where the generator is attempting to maximize its expectation while minimizing the discriminator's error rate. In other words — the Generator tries harder to fool the Discriminator and the Discriminator becomes more captious in order to not be fooled by the Generator. The training stops when the GAN is unable (within boundary conditions) to distinguish between $p_{data}$ and $p_{z}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GANs: Image retrieval for historical archives\n",
    "\n",
    "An interesting example of GANs applications is retrieving visually similar marks in “Prize Papers,” one of the most valuable archives in the field of maritime history. Adversarial nets make it easier to work with documents of historical importance containing information about the legitimacy of ship captures at sea.\n",
    "[Adversarial Training For Sketch Retrieval](https://arxiv.org/pdf/1607.02748v1.pdf)\n",
    "\n",
    "Each query contains examples of Merchant Marks — unique identification of property of a merchant, sketch-like symbols that are similar to hieroglyphs.\n",
    "\n",
    "![gan](./images/symbol_finding_GAN.jpeg)\n",
    "\n",
    "Feature representation of every mark should be obtained, but there are some problems of applying conventional machine and deep learning methods (including Convolutional neural networks):\n",
    "\n",
    "* They require a large amount of labelled images;\n",
    "* There are no labels for Merchant Marks;\n",
    "* Marks are not segmented from the dataset.\n",
    "\n",
    "This new approach shows how to extract and learn features from images of the Merchant Marks using GANs. After the representation of each mark is learned the visual search on scanned documents could be processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text translation into images\n",
    "\n",
    "It has been shown that it’s possible to use descriptive properties of natural language to generate corresponding images. We have seen this already using complex architectures. GANs can do the same and even generate image descriptions with even less effort. A method of [text translation into images](https://web.eecs.umich.edu/~honglak/nips2015-analogy.pdf) was employed in order to link text to individual images in [Generative Adversarial Text to Image Synthesis](https://arxiv.org/pdf/1605.05396.pdf)\n",
    "\n",
    "The main problem of image generation is that image distribution is multimodal. For example, there are many correct samples that correctly illustrate the description. GANs help to solve this problem.\n",
    "\n",
    "![text_to_image](./images/GAN_example.jpeg)\n",
    "\n",
    "Consider the following task of mapping the blue input dot to the green output dot (green dots are possible outputs to blue dot). The red arrow indicates the prediction error vector. This means that after some time the blue dot will be mapped to the mean of the green dots — despite the fact that this is what we wanted, this exact property causes the blurry images we are trying to predict.\n",
    "\n",
    "Generative adversarial nets don’t directly use pairs of inputs and outputs. Instead, they learn how the inputs and outputs can be paired.\n",
    "\n",
    "Some examples of generated images from text descriptions:\n",
    "\n",
    "\n",
    "<center><img src=\"./images/image_text_GAN_output.jpeg\" height=\"250\"/></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drug Discovery\n",
    "\n",
    "While others apply generative adversarial networks to images and videos, researchers from Insilico Medicine proposed proposed an [adversarial autoencoder](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5355231/) (AAE) model for identification and generation of new compounds based on available biochemical data.\n",
    "\n",
    "![aae](./images/adversarial_autoencoder.jpeg)\n",
    "\n",
    "\n",
    "The adversarial autoencoder was trained using Growth Inhibition percentage data (GI, which shows the reduction in the number of cancer cells after the treatment), with drug concentrations, and SMILES chemical fingerprints (shorthand for chemical structure) as inputs. \n",
    "\n",
    "![smiles](./images/smiles_predictions.jpeg)\n",
    "\n",
    "The output fingerprint of the molecule contains a fixed number of bits in which each bit represents the absence or presence of some feature. The encoder was restricted to map the same fingerprint to the same latent vector.\n",
    "\n",
    "The trained AAE model was able to predict already proven anticancer agents and generated new untested compounds to be evaluated for anticancer activity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
