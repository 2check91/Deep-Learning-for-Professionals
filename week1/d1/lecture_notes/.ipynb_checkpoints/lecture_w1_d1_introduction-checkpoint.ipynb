{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Practitioners w1 d1\n",
    "\n",
    "\n",
    "## Welcome!\n",
    "\n",
    "### By the End of this Session You Will be Able To\n",
    "\n",
    "* Identify the subdisciplines within machine learning\n",
    "* Describe the steps required to construct a machine learning model\n",
    "* Describe the basics of logistic regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Machine Learning?\n",
    "\n",
    "\n",
    "\"A field of study that gives computers the ability to learn without being explicitly programmed.\" - Arthur Samuel\n",
    "\n",
    "\"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.\" - Tom M. Mitchell\n",
    "\n",
    "\"A morphism $Q \\rightarrow P$ from a set of predictors $Q \\in A$ and responses $R \\in B$ to a set of parameters $P \\in T$ such that we may construct a mapping $F: T \\rightarrow B$ representative of the original predictors.\n",
    "\n",
    "### Some truisms about machine learning\n",
    "\n",
    "#### 1. [Machine learning is not data science, and it is not the most important part of data science.](http://sharpsightlabs.com/blog/machine-learning-prerequisite-isnt-math/)\n",
    "#### 2. Machine learning does not cure badly collected data or incorrect data chosen for the situation.\n",
    "#### 3. Machine learning is *not* a substitute for experience, knowledge, or understanding.\n",
    "#### 4. Machine learning is not mostly computer science. It is mostly statistics (and used to be called \"statistical learning\" until academia got wise to the hype cycle)\n",
    "\n",
    "\n",
    "### Subdisciplines within Machine Learning\n",
    "\n",
    "### [Supervised Learning](https://en.wikipedia.org/wiki/Supervised_learning)\n",
    "Models that infer the mapping $F: T \\rightarrow B$ from *labeled* training data. The data is considered to be a fundamentally correct (!) set of examples that the model is then asked to reproduce.  \n",
    "\n",
    "#### Discriminitave Models\n",
    "Models that attempts to learn the dependent probability distribution p(y|X) without conditionals.\n",
    "\n",
    "#### Linear Models\n",
    "A group of discriminative models that attempt to model the system with linear response $y$ to predictors $X$. \n",
    "    \n",
    "#### Generative Models\n",
    "Models that attempt to learn the joint probability distribution p(y,x) (implicitly with conditionals).\n",
    "\n",
    "#### Dimensionality Reduction\n",
    "Attempts to model the response $y$ in terms of a smaller set of artifical predictors $Z$ such that $dim(Z) \\lt dim(X)$. These are obtained by learning the morphism $F: Z \\rightarrow X$\n",
    "\n",
    "#### Noise Isolation\n",
    "Models the system with additional noise terms, i.e. as $y = f(X + \\zeta) + (g_0(\\epsilon) + \\ldots g_1(\\epsilon))$ with the hope of being to extract true dependency between $y$ and $X$ in terms of parameters\n",
    "\n",
    "### [Unsupervised Learning](https://en.wikipedia.org/wiki/Unsupervised_learning)\n",
    "Models that infer the mapping $F: T \\rightarrow B$ from *unlabeled* training data. The hidden structure of the data is represented in terms of a set of hyperparameters obtained from the design of the model. \n",
    "\n",
    "### [Semisupervised Learning](https://en.wikipedia.org/wiki/Semi-supervised_learning)\n",
    "Like supervised learning, but only some examples are provided. \n",
    "\n",
    "### [Active Learning](https://en.wikipedia.org/wiki/Active_learning_%28machine_learning%29)\n",
    "The model actively seeks additional input and changes parameters over time as new information comes in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Assigned Reading: Generative vs. Discriminative Models](../readings/2020-on-discriminative-vs-generative-classifiers-a-comparison-of-logistic-regression-and-naive-bayes.pdf)\n",
    "\n",
    "\n",
    "<img src=\"images/discriminative_vs_generative.png\">\n",
    "[source](http://www.evolvingai.org/fooling)\n",
    "<img src=\"images/generative-discriminative.jpg\">\n",
    "[source](http://enginius.tistory.com/461)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Fundamentals of Machine Learning Implementation\n",
    "\n",
    "1. Hypothesis\n",
    "2. Cost Function\n",
    "3. Estimation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "## Logistic Regression Motivation: The Best Defense is ....\n",
    "\n",
    "Let's start by looking at an example. We're going to be using some NFL data. The x axis is the number of touchdowns scored by team over a season and the y axis is whether they lost or won the game indicated by a value of 0 or 1 respectively.\n",
    "\n",
    "![NFL data](images/nfl.png)\n",
    "\n",
    "So, how do we predict whether we have a win or a loss if we are given a score? Note that we are going to be predicting values between 0 and 1. Close to 0 means we're sure it's in class 0, close to 1 means we're sure it's in class 1, and closer to 0.5 means we don't know.\n",
    "\n",
    "If we use linear regression for the NFL example above, we will certainly do better than randomly guessing, but it doesn't accurately represent the data:\n",
    "\n",
    "![NFL linear regression](images/linefit.png)\n",
    "\n",
    "So clearly a line is not the best way to model this data. So we need to find a better curve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Logistic Function\n",
    "\n",
    "First, we will just pull a function out of the data science bag of tricks and show that it works reasonably well.\n",
    "\n",
    "And, second, we are going to understand how we came up with that function and how it is related to binary outcomes and odds. But before that let's understand this a bit better.\n",
    "\n",
    "This function will need to have a value of 0 for the loss scores and 1 for the win scores. To make sense it will need to be 0 for some score and all scores below it and be 1 for some other score and all scores above it. And it will need to smoothly increase from 0 to 1 in the intermediate range.\n",
    "\n",
    "It will need to look something like this:\n",
    "\n",
    "![logistic](images/standardLogisticFunction.png)\n",
    "\n",
    "\n",
    "A function that has the above shape is:\n",
    "\n",
    "$$ f(t) = \\frac{e^t}{1 + e^t} = \\frac{1}{1 + e^{-t}} $$\n",
    "\n",
    "This is the logistic function, also known as the sigmoid function. Note that as t approaches infinity, the value of the logistic function approaches 1 and as t approaches negative infinity, the value of the logistic function approaches 0.\n",
    "\n",
    "We will use $ t = \\beta_0 + \\beta_1x $, which means we'll be dealing with a familiar looking linear function.\n",
    "\n",
    "This gives us:\n",
    "\n",
    "$$ p(x) = \\frac{e^{\\beta_0 + \\beta_1 x}}{1 + e^{\\beta_0 + \\beta_1 x}} = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x)}} $$\n",
    "\n",
    "* p(x) is our hypothesis and it represents the probability of a score of x leading to a win. \n",
    "\n",
    "\\begin{align*}\n",
    "& P(Y = 1 | X) = p(X) \\\\\n",
    "& P(Y = 0 | X) = 1 - p(X)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "* $\\beta_0$ and $\\beta_1$ are parameters that we will optimize to best fit our data.\n",
    "\n",
    "Also note that this is just for the one variable case, but we can similarly plug in the linear equation for any size feature matrix.\n",
    "\n",
    "This is not necessarily the \"best possible\" function, but there's two main reasons we use the logistic/sigmoid function:\n",
    "\n",
    "1. It has the correct shape\n",
    "2. It makes the math easy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Estimation\n",
    "\n",
    "We choose $\\bf{\\beta}$ by maximizing the likelihood of the data.\n",
    "\n",
    "**What is the likelihood of seeing a particular data point $(x_i, y_i)$?** Keep in mind that $y_i\\in\\{0,1\\}$\n",
    "\n",
    "$$L(\\beta) = P(y_i | x_i) = p(x_i)^{y_i} \\times (1 - p(x_i))^{1 - y_i}$$\n",
    "\n",
    "**The likelihood of all data points** (We are going to calculate the *likelihood* that we predict all the data points correctly. This is calculated by taking the product of all the individual likelihoods for each data point in our training set.)\n",
    "\n",
    "$$L(\\beta) = \\prod_{i = 1}^{n} p(x_i)^{y_i} \\times (1 - p(x_i))^{1 - y_i}$$\n",
    "\n",
    "For computational reasons, we will be maximizing the log of this function instead. Maximizing the **log-likelihood** is equivalent, but will make computations easier. One issue with calculating small probabilities with a computer is *numerical underflow*. Once the values get sufficiently small, they will be rounded to 0 and we will lose all information.\n",
    "\n",
    "Using properties of log, we can simplify the formula: \n",
    "\n",
    "\\begin{align*}\n",
    "l(\\beta) & = log(L(\\beta)) \\\\\n",
    "         & = log \\left( \\prod_{i = 1}^{n} p(x_i)^{y_i} \\times (1 - p(x_i))^{1 - y_i} \\right) \\\\\n",
    "         & = \\sum_{i = 1}^{n} log(p(x_i)^{y_i} \\times (1 - p(x_i))^{1 - y_i}) \\\\\n",
    "         & = \\sum_{i = 1}^{n} y_i log(p(x_i)) + (1 - y_i) log(1 - p(x_i)) \\\\\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent with Logistic Regression\n",
    "\n",
    "Just like we do in linear regression, we can use gradient descent to find the optimal coefficients. Recall the [Gradient Descent](http://www.onmyphd.com/?p=gradient.descent) algorithm:\n",
    "\n",
    "    Gradient Descent:\n",
    "        input: J: optimization function (cost function)\n",
    "               alpha: learning rate\n",
    "               n: number of iterations\n",
    "        output: local minimum of optimization function J\n",
    "\n",
    "        initialize b (often as all 0's)\n",
    "        repeat for n iterations:\n",
    "            update b as b - alpha * gradient(J)\n",
    "\n",
    "The gradient descent algorithm requires us to choose a cost function and calculate the gradient of the cost function.\n",
    "\n",
    "#### Cost function\n",
    "\n",
    "Maximizing the likelihood or the log-likelihood is equivalent to minimizing the negative log-likelihood of the data, i.e., the total cost.\n",
    "\n",
    "$$Total \\  cost = - \\left( \\sum_{i = 1}^{n} y_i log(p(x_i)) + (1 - y_i) log(1 - p(x_i)) \\right) $$\n",
    "\n",
    "We define the logistic regression cost function as the average cost,\n",
    "\n",
    "$$ J(\\beta) = - \\frac{1}{n} \\left( \\sum_{i = 1}^{n} y_i log(p(x_i)) + (1 - y_i) log(1 - p(x_i)) \\right) $$\n",
    "\n",
    "#### Gradient of the cost function\n",
    "Recall that to use gradient descent, we need to know the gradient of the cost function. This is where we see the advantages of the logistic function. First, let's just calculate the derivative of the logistic function:\n",
    "\n",
    "\\begin{align*}\n",
    "f'(t) & = \\frac{d}{dt} \\frac{1}{1 + e^{-t}} \\\\\n",
    "      & = \\frac{d}{dt} \\frac{e^{t}}{1 + e^{t}} \\\\\n",
    "      & = \\frac{e^{t}(1+e^{t})-e^{t}e^{t}}{(1 + e^{t})^2}\\\\\n",
    "      & = \\frac{e^{t}}{(1 + e^{t})^2} \\\\\n",
    "      & = f(t)(1 - f(t)) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Using the chain rule for fractions in step 3. Isn't it nice how that works out?\n",
    "\n",
    "Now we'll take the partial derivative of the cost function for each coefficient:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial \\beta_j} J(\\beta) \n",
    "    & = -\\frac{1}{n} \\cdot \\frac{\\partial}{\\partial \\beta_j} \\left( \\sum_{i=1}^{n} y_i log(p(x_i)) + (1 - y_i) log(1 - p(x_i)) \\right) \\\\\n",
    "    & = -\\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\beta_j} (y_i log(p(x_i)) + (1 - y_i) log(1 - p(x_i))) \\\\\n",
    "    & = -\\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i \\cdot \\frac{1}{p(x_i)} \\cdot \\frac{\\partial}{\\partial \\beta_j} p(x_i) + (1 - y_i) \\cdot \\frac{1}{1 - p(x_i)} \\cdot \\frac{\\partial}{\\partial \\beta_j} (1 - p(x_i)) \\right) \\\\\n",
    "    & = -\\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i \\cdot \\frac{1}{p(x_i)} - (1 - y_i) \\cdot \\frac{1}{1 - p(x_i)} \\right) \\frac{\\partial}{\\partial \\beta_j} p(x_i) \\\\\n",
    "    & = -\\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i \\cdot \\frac{1}{p(x_i)} - (1 - y_i) \\cdot \\frac{1}{1 - p(x_i)} \\right) p(x_i) (1 - p(x_i)) \\frac{\\partial}{\\partial \\beta_j} (\\beta_0 + \\beta_1 x_i) \\\\\n",
    "    & = -\\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i (1 - p(x_i)) - (1 - y_i) p(x_i) \\right) x_{ij} \\\\\n",
    "    & = -\\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i - p(x_i) \\right) x_{ij} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Where $x_{ij} = 1$ if $j = 0$ and $x_{ij} = x_i$ if $j = 1$ in the single variable case.\n",
    "\n",
    "This is what we'll use to update the coefficients in each iteration of gradient descent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Interpretation\n",
    "\n",
    "We define the odds of Y = 1 as,\n",
    "$$odds = \\frac{P(Y = 1)}{1 - P(Y = 1)} = e^{\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p}$$\n",
    "\n",
    "We interpret the $\\beta's$ in multiplicative terms with respect to the odds.\n",
    "\n",
    "E.g., the interpretation of $\\beta_1$ is, holding all the other variables/features fixed, for every increase of 1 in $X_1$, the odds of $Y = 1$ increases by a factor of $e^{\\beta_1}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization in Logistic Regression\n",
    "\n",
    "Logistic regression is immensely powerful in many cases but in particular tends to over-fit when there are many features. Weights tend to become skewed in the OLS optimum, partially due to the effect of the logit function on the gradient. Just as in linear regression, the effect can be mitigated using regularization, penalizing the model for using additional predictors.\n",
    "\n",
    "Recall that the overall cost of the data is \n",
    "$$Total \\  cost = - \\left( \\sum_{i = 1}^{n} y_i log(p(x_i)) + (1 - y_i) log(1 - p(x_i)) \\right) $$\n",
    "\n",
    "**Lasso Regularization** ($L_{1}$) Effective in eliminating redundant predictors. This sometimes leads to a better production (!) regression.\n",
    "$$Total \\  cost = - \\left( \\sum_{i = 1}^{n} y_i log(p(x_i)) + (1 - y_i) log(1 - p(x_i)) \\right) + \\lambda \\sum_{j = 1}^{p} |\\beta_j|$$\n",
    "\n",
    "**Ridge Regularization** ($L_{2}$) Standard shrinkage when you want to keep predictors and believe that there is collinearity present.\n",
    "\n",
    "$$Total \\  cost = - \\left( \\sum_{i = 1}^{n} y_i log(p(x_i)) + (1 - y_i) log(1 - p(x_i)) \\right) + \\lambda \\sum_{j = 1}^{p} \\beta_j^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
