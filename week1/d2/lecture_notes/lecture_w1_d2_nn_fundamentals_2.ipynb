{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Building Simple Neural Networks (Pseudologistic regression)\n",
    "\n",
    "with help from these [notebooks](https://github.com/stephencwelch/Neural-Networks-Demystified)\n",
    "\n",
    "By The End Of This Session You Should Be Able To:\n",
    "----\n",
    "\n",
    "- Fit data with a pseudologistic regression/ perceptron model\n",
    "- Remember the power of matrix algebra and calculus \n",
    "- Explain the fundamentals of gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following on from the previous lectures, it's time to apply the ideas we've covered into a working machine learning model. Let's first spend a bit of time on the data we'll need to illustrate the idea. We will study the performance of a given student on a test (say me) with respect to hours sleeping and hours studying (the two main inputs of student performance...?). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X = (hours sleeping, hours studying)\n",
    "X = np.array(([3,5], [5,1], [10,2]), dtype=float)\n",
    "y = np.array(([75], [82], [93]), dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some data, we’re going to use it to train a model to predict how you will do on your next test, based on how many hours you sleep and how many hours you study.\n",
    "\n",
    "\n",
    "### Review\n",
    "* What assumptions does the above statement make with respect to the design of this problem?\n",
    "* What type of machine learning problem is this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEVCAYAAAAPRfkLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9171123d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEVCAYAAAAPRfkLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOSmYmVnOScHMzHJOCmZmlnNSMDOz3P8BqMXCEY/V/iAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f916f871f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# A little data exploration never hurt\n",
    "\n",
    "sns.regplot(x=X[:,0],\n",
    "           y=y,\n",
    "           fit_reg=False);\n",
    "plt.title(\"The effect of sleeps hours on test scores\");\n",
    "plt.xlabel('Sleep hours');\n",
    "plt.ylabel('Test Scores');\n",
    "plt.show()\n",
    "sns.regplot(x=X[:,1],\n",
    "           y=y,\n",
    "           fit_reg=False);\n",
    "plt.title(\"The effect of study hours on test scores\");\n",
    "plt.xlabel('Study hours');\n",
    "plt.ylabel('Test Scores');\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "Normalization\n",
    "------\n",
    "\n",
    "Before we throw our data into the model, we need to account for differences in the units of our data. Both of our inputs are in hours, but our output is a test score, scaled between 0 and 100. \n",
    "\n",
    "Why do we do this? It is standard practice in most machine learning models. Choosing not to do it is akin to asking our model to compare apples to oranges. Models in general are set up to derive comparisons of features **relative to themselves**, (thus apples to apples). The solution is to scale the data to a uniform standard. \n",
    "\n",
    "Here, we're going to take advantage of the fact that all of our data is positive, and simply divide by the maximum value for each variable, effectively scaling the result between 0 and 1. In more complex applications, you can use scikit's [StandardScaler()](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html), or write something similar yourself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Network Design\n",
    "\n",
    "![](images/simpleNetwork.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A word about the above design\n",
    "\n",
    "<h3 align = 'center'> Variables </h3>\n",
    "\n",
    "|Code Symbol | Math Symbol | Definition | Dimensions\n",
    "| :-: | :-: | :-: | :-: |\n",
    "|X|$$X$$|Input Data, each row is an example| (numExamples, input_layer_size)|\n",
    "|y |$$y$$|target data|(numExamples, output_layer_size)|\n",
    "|yHat | $$\\hat{y}$$|predicted value|(numProblems, output_layer_size)|\n",
    "\n",
    "-----\n",
    "#### Parameters & hyperparameters\n",
    "----\n",
    "\n",
    "Our neural network's __parameters__ are the weights between the nodes.\n",
    "\n",
    "Our network's __hyperparameters__ are the 2 inputs, 3 hidden units, and 1 output. \n",
    "\n",
    "In general, paramters change for each dataset. Hyperparameters do __not__ change across minor changes in datasets.\n",
    "\n",
    "Today, we'll learn just the parameters. (Later we'll also learn the hyperparameters.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning how to Predict (Training the Network)\n",
    "\n",
    "We will now introduce you to backpropagation, the process of iteratively updating the weights of a neural network. This differs from typical model optimization, because the system of neurons does not learn as a whole. Each neuron learns its parameters with respect to other neurons. Thus optimization does not take place in terms of a smooth linear descent down a loss function gradient, but instead contains a great deal of noise and change, as each neuron in the network learns to cooperate with its neighbors. This means for very large (deep) NN, a great deal of complexity is involved in optimization. \n",
    "\n",
    "\n",
    "[Backpropagation]()  takes place in 3 steps:\n",
    "1. Forward propagation - The network makes a prediction based on the current weights of each neuron.\n",
    "2. Backward propagation - The network updates the gradient based on information returned by the loss function.\n",
    "3. Update - Each neuron's weights are updated according to the gradient.\n",
    "\n",
    "Pseudocode\n",
    "\n",
    "      initialize network weights (often small random values)\n",
    "      while stopping_criterion not satisfied\n",
    "         for (training example) ex in training_samples\n",
    "            prediction = neural-net-output(network, ex)  // forward pass\n",
    "            actual = teacher-output(ex)\n",
    "            compute error (prediction - actual) at the output units\n",
    "            compute Δ w h for all weights from hidden layer to output layer \n",
    "            compute Δ w i for all weights from input layer to hidden layer \n",
    "            update network weights // input layer not modified by error estimate\n",
    "\n",
    "\n",
    "\n",
    "Each input value, or element in matrix X, needs to be multiplied by a corresponding weight and then added together with all the other results for each neuron. This is a complex operation, but if we take the three outputs we're looking for as a single row of a matrix, and place all our individual weights into a matrix of weights, we can create the exact behavior we need by multiplying our input data matrix by our weight matrix. \n",
    "\n",
    "Using matrix multiplication allows us to pass multiple inputs through at once by simply adding rows to the matrix X. From here on out, we'll refer to these matrices as X, W one, and z two, where z two the activity of our second layer. \n",
    "\n",
    "-----\n",
    "\n",
    "<h3 align = 'center'> Variables So Far</h3>\n",
    "\n",
    "|Code Symbol | Math Symbol | Definition | Dimensions\n",
    "| :-: | :-: | :-: | :-: |\n",
    "|X|$$X$$|Input Data, each row in an example| (numExamples, input_layer_size)|\n",
    "|y |$$y$$|target data|(numExamples, output_layer_size)|\n",
    "|yHat | $$\\hat{y}$$|predicted value|(numProblems, output_layer_size)|\n",
    "|W1 | $$W^{(1)}$$ | Layer 1 weights | (input_layer_size, hidden_layer_size) |\n",
    "|W2 | $$W^{(2)}$$ | Layer 2 weights | (hidden_layer_size, output_layer_size) |\n",
    "|z2 | $$z^{(2)}$$ | Layer 2 activation | (numExamples, hidden_layer_size) |\n",
    "|a2 | $$a^{(2)}$$ | Layer 2 activity | (numExamples, hidden_layer_size) |\n",
    "|z3 | $$z^{(3)}$$ | Layer 3 activation | (numExamples, output_layer_size) |\n",
    "\n",
    "\n",
    "Notice that each entry in z is a sum of weighted inputs to each hidden neuron. Z is of size 3 by 3, one row for each example, and one column for each hidden unit. Matrix notation is really nice here, because it allows us to express the complex underlying process in a single line!\n",
    "\n",
    "$$\n",
    "z^{(2)} = XW^{(1)} \\tag{1}\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Activation Function\n",
    "\n",
    "Once the inputs are obtained (from $W$) for our second layer, $z^{(2)}$, we need to apply the activation function. \n",
    "\n",
    "We'll independently apply the function to each entry in matrix z using a python method for this called sigmoid, because we’re using a sigmoid as our activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here is an example of how this is going to work in python\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"Define sigmoid activation function to scalar, vector, or matrix\"\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "sigmoid(np.random.randn(3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Forward propagation\n",
    "----\n",
    "\n",
    "Formula for forward propagation, using f to denote our activation function.\n",
    "\n",
    "We can write that $a^{(2)}$, our second layer activity, is equal to f of $z^{(2)}$. $a^{(2)}$ will be a matrix of the same size as $z^{(2)}$, 3 by 3.\n",
    "\n",
    "$$\n",
    "a^{(2)} = f(z^{(2)}) \\tag{2}\\\\\n",
    "$$\n",
    "\n",
    "To finish forward propagation we need to propagate $a^{(2)}$ all the way to the output, $\\hat{y}$. We've already done the heavy lifting in the previous layer, so all we have to do now is multiply $a^{(2)}$ by our second layer weights $W^{(2)}$ and apply one more activation function. W2 will be of size 3x1, one weight for each synapse. Multiplying $a^{(2)}$ , a 3 x 3 matrix, by $W^{(2)}$ , a 3 x 1 matrix results in a 3 x 1 matrix $z^{(3)}$, the activity or our third layer. $z^{(3)}$ has three activity values, one for each example. \n",
    "\n",
    "Finally, we'll apply our activation function to $z^{(3)}$ yielding our official estimate of your test score, $\\hat{y}$.  \n",
    "\n",
    "$$\n",
    "z^{(3)} = a^{(2)}W^{(2)} \\tag{3}\\\\\n",
    "$$\n",
    "$$\n",
    "\\hat{y} = f(z^{(3)}) \\tag{4}\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course the predictions from this network are going to be pointlessly bad, as the weights are random at this point. We need to create a way by which we can optimize the weights cleanly \n",
    "\n",
    "---\n",
    "## Loss & Cost Function\n",
    "----\n",
    "\n",
    "When someone says they’re training a network, what they really mean is that they're minimizing a **cost** function. \n",
    "\n",
    "**Loss:** A quantification of prediction error\n",
    "\n",
    "**Cost:** A quantification of prediction error with respect to the overall objectives of the model\n",
    "\n",
    "**Objective:** Any function to be optimized in machine learning\n",
    "\n",
    "The standard approach to adding in loss to machine learning cost functions is to use a squared loss:\n",
    "\n",
    "$$\n",
    "J = \\sum \\frac{1}{2}(y-\\hat{y})^2 \\tag{5}\n",
    "$$\n",
    "\n",
    "And this is what we are going to do in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation I: Forward Propagation\n",
    "\n",
    "$$\n",
    "z^{(2)} = XW^{(1)} \\tag{1}\\\\\n",
    "$$\n",
    "$$\n",
    "a^{(2)} = f(z^{(2)}) \\tag{2}\\\\\n",
    "$$\n",
    "$$\n",
    "z^{(3)} = a^{(2)}W^{(2)} \\tag{3}\\\\\n",
    "$$\n",
    "$$\n",
    "\\hat{y} = f(z^{(3)}) \\tag{4}\\\\\n",
    "$$\n",
    "$$\n",
    "J = \\sum \\frac{1}{2}(y-\\hat{y})^2 \\tag{5}\\\\\n",
    "$$\n",
    "\n",
    "\n",
    "At this point, we can implement all of the above vector equations and ideas in a class structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"Simple neural network\"\n",
    "    \n",
    "    def __init__(self):  \n",
    "        # Network architecture      \n",
    "        self.input_layer_size  = 2\n",
    "        self.hidden_layer_size = 3\n",
    "        self.output_layer_size = 1\n",
    "        \n",
    "        # Weights \n",
    "        self.W1 = np.random.randn(self.input_layer_size, self.hidden_layer_size)\n",
    "        self.W2 = np.random.randn(self.hidden_layer_size, self.output_layer_size)\n",
    "\n",
    "    def cost_function(self, X, y):\n",
    "        \"Compute cost for given X,y, use weights already stored in class.\"\n",
    "        self.y_hat = self.forward(X)\n",
    "        J = 0.5*sum((y-self.y_hat)**2)\n",
    "        return J\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"Propagates inputs forward though network\"\n",
    "        self.z2 = np.dot(X, self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        yHat = self.sigmoid(self.z3) \n",
    "        return yHat\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        \"Define sigmoid activation function to scalar, vector, or matrix\"\n",
    "        return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a relatively complex system of 5 equations, however, since they are chained together, we can think of them as a single big equation. \n",
    "\n",
    "And since we have one big equation that uniquely determines our cost, $J$, from $X$, $y$, $W^{(1)}$, and $W^{(2)}$, we can use our good friend calculus to find what we're looking for.\n",
    "\n",
    "We want to know \"which way is downhill\", that is, what is the rate of change of $J$ with respect to $W$, also known as the derivative. And in this case, since we’re just considering one weight at a time, the partial derivative. \n",
    "\n",
    "We can derive an expression for $\\dfrac{\\partial{J}}{\\partial{W}}$, that will give us the rate of change of J with respect to W, for any value of W! If $\\dfrac{\\partial{J}}{\\partial{W}}$ is positive, then the cost function is going uphill (regardless of its shape or level of complication). If $\\dfrac{\\partial{J}}{\\partial{W}}$ is negative, the cost function is going downhill (and thus getting better). \n",
    "\n",
    "Now we can really speed things up. Since we **know beforehand** in which direction the cost decreases, we can save all that time we would have spent searching in the wrong direction. We can save even more computational time by iteratively taking steps downhill and stopping when the cost stops getting smaller. This is of course, the familiar [**gradient descent**](http://www.onmyphd.com/?p=gradient.descent).\n",
    "\n",
    "In order to employ gradient descent on this system, we need to use the same approach as we did for logistic regression. Our hueristic is this:\n",
    "\n",
    "As the optimization continues over step $k$, the weights need to change in the direction that **minimizes** cost:\n",
    "\n",
    "$$\\Delta W_{k} = \\lambda \\dfrac{\\partial{J}}{\\partial{W_k}}$$\n",
    "\n",
    "The gradient descent step line would then be as follows:\n",
    "\n",
    "$$W_{k+1} = W_{k} - \\lambda \\dfrac{\\partial{J}}{\\partial{W_k}}$$\n",
    "\n",
    "The task is now to derive a function that provides these values for $\\dfrac{\\partial{J}}{\\partial{W_k}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 2: Backpropagation and Gradient Descent\n",
    "\n",
    "\n",
    "<h3 align = 'center'> Variables </h3>\n",
    "\n",
    "|Code Symbol | Math Symbol | Definition | Dimensions\n",
    "| :-: | :-: | :-: | :-: |\n",
    "|X|$$X$$|Input Data, each row in an example| (numExamples, inputLayerSize)|\n",
    "|y |$$y$$|target data|(numExamples, outputLayerSize)|\n",
    "|W1 | $$W^{(1)}$$ | Layer 1 weights | (inputLayerSize, hiddenLayerSize) |\n",
    "|W2 | $$W^{(2)}$$ | Layer 2 weights | (hiddenLayerSize, outputLayerSize) |\n",
    "|z2 | $$z^{(2)}$$ | Layer 2 activation | (numExamples, hiddenLayerSize) |\n",
    "|a2 | $$a^{(2)}$$ | Layer 2 activity | (numExamples, hiddenLayerSize) |\n",
    "|z3 | $$z^{(3)}$$ | Layer 3 activation | (numExamples, outputLayerSize) |\n",
    "|J | $$J$$ | Cost | (1, outputLayerSize) |\n",
    "|dJdz3 | $$\\frac{\\partial J}{\\partial z^{(3)} } = \\delta^{(3)}$$ | Partial derivative of cost with respect to $z^{(3)}$ | (numExamples,outputLayerSize)|\n",
    "|dJdW2|$$\\frac{\\partial J}{\\partial W^{(2)}}$$|Partial derivative of cost with respect to $W^{(2)}$|(hiddenLayerSize, outputLayerSize)|\n",
    "|dz3dz2|$$\\frac{\\partial z^{(3)}}{\\partial z^{(2)}}$$|Partial derivative of $z^{(3)}$ with respect to $z^{(2)}$|(numExamples, hiddenLayerSize)|\n",
    "|dJdW1|$$\\frac{\\partial J}{\\partial W^{(1)}}$$|Partial derivative of cost with respect to $W^{(1)}$|(inputLayerSize, hiddenLayerSize)|\n",
    "|delta2|$$\\delta^{(2)}$$|Backpropagating Error 2|(numExamples,hiddenLayerSize)|\n",
    "|delta3|$$\\delta^{(3)}$$|Backpropagating Error 1|(numExamples,outputLayerSize)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our weights, W, are spread across two matrices, $W^{(1)}$, the input layer, and $W^{(2)}$, the middle or hidden layer. We’ll separate our dJ/dW computation in the same way, by computing dJdW1 and dJdW2 independently. We should have just as many gradient values as weight values, so when we’re done, our matrices $\\dfrac{\\partial{J}}{\\partial{W^{(1)}}}$ and  $\\dfrac{\\partial{J}}{\\partial{W^{(2)}}}$ will be the same size as  $W^{(1)}$ and  $W^{(2)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rate of change of hidden layer $ \\frac{\\partial J}{\\partial W^{(2)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work on the hidden layer first, starting with the cost function itself:\n",
    "    \n",
    "$$ \\frac{\\partial J}{\\partial W^{(2)}} = \\frac{\\partial \\sum \\frac{1}{2}(y-\\hat{y})^2}{\\partial W^{(2)}} = \\sum \\frac{\\partial \\frac{1}{2}(y-\\hat{y})^2}{\\partial W^{(2)}} \\tag{6}$$\n",
    "\n",
    "(Calculus rule - derivative of sums is the sum of derivatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial J}{\\partial W^{(2)}} = -(y-\\hat{y}) \\frac{\\partial \\hat{y}}{\\partial W^{(2)}} \\tag{7}\n",
    "$$\n",
    "(Calculus rule - chain rule. $\\frac{\\partial y}{\\partial W^{(2)}} = 0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because $\\hat{y}$ is a function of $z^{(3)}$ (equation 4), we need to break down the right side partial derivative into its chain dependency:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial W^{(2)}} = \n",
    "-(y-\\hat{y})\n",
    "\\frac{\\partial \\hat{y}}{\\partial z^{(3)}}  \n",
    "\\frac{\\partial z^{(3)}}{\\partial W^{(2)}} \\tag{8}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we need to add the derivative of the sigmoid function:\n",
    "\n",
    "$$\n",
    "f(z) = \\frac{1}{1+e^{-z}} \\\\ \n",
    "\\frac{\\partial f}{\\partial z} = \\frac{e^{-z}}{(1+e^{-z})^2} \\tag{9}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining these together, we are getting close to a single function:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial W^{(2)}}= \n",
    "-(y-\\hat{y}) \\frac{\\partial f(z^{(3)})}{\\partial z} \\frac{\\partial z^{(3)}}{\\partial W^{(2)}} \\tag{10}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last partial derivative can be replaced with a function if we look back at equation 3:\n",
    "\n",
    "$$\n",
    "z^{(3)} = a^{(2)}W^{(2)} \\tag{3}\\\\\n",
    "$$\n",
    "\n",
    "Again this equation simply states that $Z^{(3)}$ is the matrix product of $a^{(2)}$ and our weights from the hidden layer, $W^{(2)}$. Thus a single synapse has a  linear relationship with its weights, and thus the slope of its response is simply the activation level on that synapse: $a^{(2)}$.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z^{(3)}}{\\partial W^{(2)}} = a^{(2)} \\tag{11}\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to be careful with our dimensionality here, and if we’re clever, we can take care of that summation we got rid of earlier by using vector product,\n",
    "\n",
    "The first part of our equation, $(y-\\hat{y})$ is of the same dimension as our output data, 3x1. \n",
    "\n",
    "$\\dfrac{\\partial f(z^{(3)})}{\\partial z}$ is of the same size, 3x1, and our first operation is scalar multiplication. Each value in the resulting 3x1 matrix needs to be multiplied by each activity. We can achieve this by transposing  $a^{(2)}$ (currently a row vector) and matrix multiplying: \n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial W^{(2)}}= \n",
    "-(a^{(2)})^{T} (y-\\hat{y}) \\frac{\\partial f(z^{(3)})}{\\partial z} \\tag{12}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rate of change of input layer $ \\frac{\\partial J}{\\partial W^{(1)}}$\n",
    "\n",
    "We have one final term to compute: dJ/dW1. It begins the same, however, we now take the derivative “across” our synapses, this is a little different from our job last time, computing the derivative with respect to the weights on our synapses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial J}{\\partial W^{(1)}} = (y-\\hat{y})\n",
    "\\frac{\\partial \\hat{y}}{\\partial W^{(1)}} \\tag{13}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial W^{(1)}} = (y-\\hat{y})\n",
    "\\frac{\\partial \\hat{y}}{\\partial z^{(3)}}\n",
    "\\frac{\\partial z^{(3)}}{\\partial W^{(1)}} \\tag{14}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial W^{(1)}} = -(y-\\hat{y})\\frac{\\partial f(z^{(3)})}{\\partial z} \\frac{\\partial z^{(3)}}{\\partial W^{(1)}} \\tag{14}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z^{(3)}}{\\partial W^{(1)}} = \\frac{\\partial z^{(3)}}{\\partial a^{(2)}}\\frac{\\partial a^{(2)}}{\\partial W^{(1)}} \\tag{15}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There’s still a nice linear relationship along each synapse, but now we’re interested in the rate of change of $z^{(3)}$ with respect to $a^{(2)}$. Again using equation 3, now the slope is just equal to the weight value for that synapse. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial J}{\\partial W^{(1)}} = \\delta^{(3)} \n",
    "(W^{(2)})^{T}\n",
    "\\frac{\\partial a^{(2)}}{\\partial W^{(1)}} \\tag{16}\\\\\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial W^{(1)}} = \\delta^{(3)} \n",
    "(W^{(2)})^{T}\n",
    "\\frac{\\partial a^{(2)}}{\\partial z^{(2)}} \n",
    "\\frac{\\partial z^{(2)}}{\\partial W^{(1)}} \\tag{17}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\dfrac{\\partial a^{(2)}}{\\partial z^{(2)}} $ is exactly the same activation function as before, with the same derivative, so we can throw it in there as we did before:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial W^{(1)}} = \\delta^{(3)} \n",
    "(W^{(2)})^{T}\n",
    "\\frac{\\partial f(z^{(2)})}{\\partial z} \n",
    "\\frac{\\partial z^{(2)}}{\\partial W^{(1)}} \\tag{18}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we need a term for the last partial derivative. Looking back at equation 1, we can see that this is simply a linear relationship:\n",
    "\n",
    "\n",
    "$$\n",
    "z^{(2)} = XW^{(1)} \\tag{1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z^{(2)}}{\\partial W^{(1)}} = X \\tag{19}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial J}{\\partial W^{(1)}} = \n",
    "X^{T}\n",
    "\\delta^{(3)} \n",
    "(W^{(2)})^{T}\n",
    "f^\\prime(z^{(2)}) \\tag{20}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can upgrade our code to reflect these ideas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting neural_network.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile neural_network.py\n",
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \"Simple neural network\"\n",
    "    \n",
    "    def __init__(self):  \n",
    "        # Network architecture      \n",
    "        self.input_layer_size  = 2\n",
    "        self.hidden_layer_size = 3\n",
    "        self.output_layer_size = 1\n",
    "        \n",
    "        # Weights \n",
    "        self.W1 = np.random.randn(self.input_layer_size, self.hidden_layer_size)\n",
    "        self.W2 = np.random.randn(self.hidden_layer_size, self.output_layer_size)\n",
    "        \n",
    "    def cost_function(self, X, y):\n",
    "        \"Compute cost for given X,y, use weights already stored in class.\"\n",
    "        self.y_hat = self.forward(X)\n",
    "        J = 0.5*sum((y-self.y_hat)**2)\n",
    "        return J\n",
    "    \n",
    "    def cost_function_prime(self, X, y):\n",
    "        \"Compute derivative with respect to W and W2 for a given X and y:\"\n",
    "        self.y_hat = self.forward(X)\n",
    "        \n",
    "        delta3 = np.multiply(-(y-self.y_hat), self.sigmoid_prime(self.z3))\n",
    "        dJdW2 = np.dot(self.a2.T, delta3)\n",
    "        \n",
    "        delta2 = np.dot(delta3, self.W2.T)*self.sigmoid_prime(self.z2)\n",
    "        dJdW1 = np.dot(X.T, delta2)  \n",
    "        \n",
    "        return dJdW1, dJdW2\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"Propagate inputs though network\"\n",
    "        self.z2 = np.dot(X, self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        yHat = self.sigmoid(self.z3) \n",
    "        return yHat\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        \"Define sigmoid activation function to scalar, vector, or matrix\"\n",
    "        return 1/(1+np.exp(-z)) \n",
    "\n",
    "    def sigmoid_prime(self,z):\n",
    "        \"Gradient of sigmoid\"\n",
    "        return np.exp(-z)/((1+np.exp(-z))**2)\n",
    "    \n",
    "    #####################################################\n",
    "    # Helper Functions\n",
    "    def get_params(self):\n",
    "        \"Transform W1 and W2 unrolled into vector\"\n",
    "        params = np.concatenate((self.W1.ravel(), self.W2.ravel()))\n",
    "        return params\n",
    "    \n",
    "    def set_params(self, params):\n",
    "        \"Set W1 and W2 using single paramater vector.\"\n",
    "        W1_start = 0\n",
    "        W1_end = self.hidden_layer_size * self.input_layer_size\n",
    "        self.W1 = np.reshape(params[W1_start:W1_end], (self.input_layer_size , self.hidden_layer_size))\n",
    "        W2_end = W1_end + self.hidden_layer_size*self.output_layer_size\n",
    "        self.W2 = np.reshape(params[W1_end:W2_end], (self.hidden_layer_size, self.output_layer_size))\n",
    "        \n",
    "    def compute_gradients(self, X, y):\n",
    "        dJdW1, dJdW2 = self.cost_function_prime(X, y)\n",
    "        return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how should we change our W’s to decrease our cost? We can now compute dJ/dW, which tells us which way is uphill in our 9 dimensional optimization space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03981368,  0.00522668,  0.00419219],\n",
       "       [-0.02257864,  0.00326703,  0.0022734 ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from neural_network import *\n",
    "# X = (hours sleeping, hours studying), y = Score on test\n",
    "X = np.array(([3,5], [5,1], [10,2]), dtype=float)\n",
    "y = np.array(([75], [82], [93]), dtype=float)\n",
    "\n",
    "X /= np.amax(X, axis=0)\n",
    "y /= 100\n",
    "\n",
    "\n",
    "NN = NeuralNetwork()\n",
    "cost1 = NN.cost_function(X,y)\n",
    "dJdW1, dJdW2 = NN.cost_function_prime(X,y)\n",
    "dJdW1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at a couple iterations of the gradient descent process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scalar = 3\n",
    "dJdW1, dJdW2 = NN.cost_function_prime(X,y)\n",
    "NN.W1 = NN.W1 - scalar*dJdW1\n",
    "NN.W2 = NN.W2 - scalar*dJdW2\n",
    "cost2 = NN.cost_function(X,y)\n",
    "\n",
    "dJdW1, dJdW2 = NN.cost_function_prime(X,y)\n",
    "NN.W1 = NN.W1 - scalar*dJdW1\n",
    "NN.W2 = NN.W2 - scalar*dJdW2\n",
    "cost3 = NN.cost_function(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0978571] [ 0.03376827] [ 0.01998163]\n"
     ]
    }
   ],
   "source": [
    "print(cost1, cost2, cost3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "\n",
    "We need to train our model. We'll talk more about training soon.\n",
    "\n",
    "For today, I wrote a training class to help us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to neural_network.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a neural_network.py\n",
    "\n",
    "from scipy import optimize\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(self, N):\n",
    "        \"Make Local reference to network\"\n",
    "        self.N = N\n",
    "        \n",
    "    def callbackF(self, params):\n",
    "        self.N.set_params(params)\n",
    "        self.J.append(self.N.cost_function(self.X, self.y))   \n",
    "        \n",
    "    def costFunctionWrapper(self, params, X, y):\n",
    "        self.N.set_params(params)\n",
    "        cost = self.N.cost_function(X, y)\n",
    "        grad = self.N.compute_gradients(X,y)\n",
    "        \n",
    "        return cost, grad\n",
    "        \n",
    "    def train(self, X, y, maxiter=50):\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.J = [] # Empty list to store costs\n",
    "        \n",
    "        params0 = self.N.get_params() # Intial parameters at epoch 0\n",
    "\n",
    "        options = {'maxiter': maxiter, 'disp' : True}\n",
    "        _res = optimize.minimize(self.costFunctionWrapper,\n",
    "                                 params0,\n",
    "                                 jac=True,\n",
    "                                 method='BFGS',\n",
    "                                 args=(X, y), \n",
    "                                 options=options,\n",
    "                                 callback=self.callbackF)\n",
    "        self.N.set_params(_res.x)\n",
    "        self.optimizationResults = _res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.000068\n",
      "         Iterations: 50\n",
      "         Function evaluations: 54\n",
      "         Gradient evaluations: 54\n"
     ]
    }
   ],
   "source": [
    "from neural_network import *\n",
    "\n",
    "nn = NeuralNetwork()\n",
    "t = Trainer(nn)\n",
    "t.train(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEJCAYAAABlmAtYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8392645da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(t.J)\n",
    "plt.grid(1)\n",
    "xlabel('Iterations')\n",
    "ylabel('Cost');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y observed</th>\n",
       "      <th>y predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.750428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.829016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.922574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y observed  y predicted\n",
       "0        0.75     0.750428\n",
       "1        0.82     0.829016\n",
       "2        0.93     0.922574"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'y predicted': t.N.y_hat.flatten().tolist(),\n",
    "           'y observed': y.flatten().tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAD5pJREFUeJzt3XuMXOV5x/Gv17NBJTWwjdZ1oa2iVu6jUJqWS1s7xtjE\n5tpIVVSjRAptKJsLqUlJQ5FIUVOiQilJwKpBuZCA0qhpI+UCBAGJKxIRLFeRsRGiqfvQlrpNMIRN\nWV8SUuLL9o+dpZNlL2eHMzvzzn4/kuUzc86ceV491m9ev2f27JLx8XEkSeUa6HYBkqRXxiCXpMIZ\n5JJUOINckgpnkEtS4RoL/Yajo4d65msyQ0PHMzb2QrfL6Kh+H6PjK1+/j7Gu8Q0PL1sy075FPSNv\nNJZ2u4SO6/cxOr7y9fsYF2J8izrIJakfGOSSVDiDXJIKZ5BLUuEMckkqnEEuSYUzyCWpcAa5JBVu\nwX+ys4oPfWZnref7i8t+s9bzSVIv6ckgl9S76p5oDTYGOHzkWC3nWqyTNoMceOc7387119/IKaf8\nPM899z2uvfZq7rrr77pdlqR5Oum8dd0u4eUaA5zU/KDa/48Pd+QtXCMHLrzwYh56aBsA27d/k40b\nL+hyRZJUnUEObNx4AQ8//A0Adux4hPPOM8gllcMgB0488SSWL1/Onj3f5tixcYaHl3e7JEmqzCBv\nuuCCi7n11ps599wN3S5FkualJy92duPK85o153DzzTeyfr1BLqkszsibnnjicdasWcuyZcu6XYok\nzUtPzsgX2p13fpJvfeufuPHGD3e7FEmaN4McGBl5NyMj7+52GZLUFpdWJKlwBrkkFc4gl6TCGeSS\nVLievNhZ941v6rpRzY03Xs/69RtYs2btnMc+++yzPP/89zn11NNqeW9Jmokz8g7ZvXsne/Z8u9tl\nSFoEDHImbmP79NPfBeC5577H5ZdfOuOxu3c/yvvf/14uvfQSnnzyXwG47bZbec97RhgZ+X3uu+8e\nxsbGuOuuO/jCFz7P9u2duW2lJE0yyJnfbWyXLFnCrbfexqZNb+XBB+/nxRdfZMWKk/n4x+/kYx/7\nFJ/+9CcYGhrioovexCWXvJWzz+7B+yNL6isGOfO7je3rX/8bAAwPL+eHP/wBxx13HAcPHuCKKy7n\n6qv/mP37xxakZkma1JMXOxfafG5ju3Tp0pe2x8fHeeyxXeze/Si3334HjUaD886b+0KoJNXJGXlT\nu7exPXBgP8uX/yyNRoPt2x/m6NFjHD58mIGBAY4ePdqhaiXp//XkjLxTv9duNu3exvass36bz33u\nb7nyynexdu063vCGs/noR29i48bzueGG6znppCHOP/+ijtQsSdCjQd4NVW5je91117+0vWbN2pe+\nT/6pT332peff8pa3vbR9771frb9QdVTP/fLeBfjFvSqfQc5P3sb22Wef5YYbPviyY04//UzvkCip\nJxnkvPw2trfffkcXq+lvH/rMzlrPN9gY4HBzxlqHLbWdSVo4XuyUpMJVmpFHxBZgFTAOXJWZO1v2\nbQYuBY4Cj2bm+zpRqCRpenPOyCNiHbAyM1cDI8DWln0nANcAazPzbODUiFjVqWIlSS9XZWllA3AP\nQGbuAYaaAQ7w4+afn46IBnA88HwnCpUkTa/K0soKYFfL49Hmcwcz838j4kPAU8CPgM9n5pOznWxo\n6HgajaWzHbKghodn/rphv+ilMQ426r8sU+c5O1HfKzVZU6/0sZd72Iv9g873sJ1vrSyZ3GjOzP8M\n+BXgIPD1iPj1zHx8phePjb3Qxlt2xvDwMkZHD3W7jI7qtTHW+Q0TqP9bK3XX90q1jm9/j/Sxl3vY\na/2D+no424dAlY+vfUzMwCedDDzT3H4d8FRmfj8zfww8ApzZZp2SpDZUCfJtwCaAiDgD2JeZkx8r\ne4HXRcRPNR+fBfxb3UVKkmY259JKZu6IiF0RsQM4BmyOiMuAA5l5d0R8BPhGRBwBdmTmI50tWZLU\nqtIaeWZeO+Wpx1v2fRL4ZJ1FSZKq681LvJKkygxySSqcQS5JhTPIJalwBrkkFa6o+5H3/L2sP/en\ntZ2rNs3fMONvl5H6lzNySSqcQS5JhTPIJalwBrkkFc4gl6TCGeSSVDiDXJIKZ5BLUuEMckkqnEEu\nSYUzyCWpcAa5JBXOIJekwhnkklQ4g1ySCmeQS1LhDHJJKpxBLkmFM8glqXAGuSQVziCXpMIZ5JJU\nOINckgpnkEtS4QxySSqcQS5JhTPIJalwBrkkFc4gl6TCNaocFBFbgFXAOHBVZu5s2fcLwD8ArwJ2\nZ+YVnShUkjS9OWfkEbEOWJmZq4ERYOuUQ24BbsnM3wKORsQv1l+mJGkmVZZWNgD3AGTmHmAoIk4A\niIgBYC3wleb+zZn53x2qVZI0jSpLKyuAXS2PR5vPHQSGgUPAlog4A3gkMz8w28mGho6n0VjaVrGD\njfqX9Os8Zyfqq8NgY4Dh4WXdLgOwh+2YrMkeLtx56tbpHlZaI59iyZTtU4C/AfYC90fE72Tm/TO9\neGzshTbecsLhI8fafu10BhsDtZ6z7vrqMDnG/aOHul0KYA/nq3V89nBuvdY/qK+Hs30IVPn42sfE\nDHzSycAzze3vA/+Vmf+RmUeBh4BfbbNOSVIbqgT5NmATQHP5ZF9mHgLIzCPAUxGxsnnsmUB2olBJ\n0vTmXFrJzB0RsSsidgDHgM0RcRlwIDPvBt4HfKZ54fMJ4L5OFixJ+kmV1sgz89opTz3esu/fgbPr\nLEqSVF1vXuKVJFVmkEtS4QxySSqcQS5JhTPIJalwBrkkFc4gl6TCGeSSVDiDXJIKZ5BLUuEMckkq\nnEEuSYUzyCWpcAa5JBXOIJekwhnkklQ4g1ySCmeQS1LhDHJJKpxBLkmFM8glqXAGuSQVziCXpMIZ\n5JJUOINckgpnkEtS4QxySSqcQS5JhTPIJalwBrkkFc4gl6TCGeSSVDiDXJIKZ5BLUuEMckkqnEEu\nSYVrVDkoIrYAq4Bx4KrM3DnNMTcBqzNzfa0VSpJmNeeMPCLWASszczUwAmyd5phTgXPqL0+SNJcq\nSysbgHsAMnMPMBQRJ0w55hbgupprkyRVUGVpZQWwq+XxaPO5gwARcRnwMLC3yhsODR1Po7F0XkVO\nGmzUv6Rf5zk7UV8dBhsDDA8v63YZgD1sx2RN9nDhzlO3Tvew0hr5FEsmNyLiZ4A/BDYCp1R58djY\nC2285YTDR461/drpDDYGaj1n3fXVYXKM+0cPdbsUwB7OV+v47OHceq1/UF8PZ/sQqPLxtY+JGfik\nk4FnmttvBIaBR4C7gTOaF0YlSQukSpBvAzYBRMQZwL7MPASQmV/MzFMzcxXwZmB3Zv5Jx6qVJL3M\nnEGemTuAXRGxg4lvrGyOiMsi4s0dr06SNKdKa+SZee2Upx6f5pi9wPpXXpIkaT568xKvJKkyg1yS\nCmeQS1LhDHJJKpxBLkmFM8glqXAGuSQVziCXpMIZ5JJUOINckgpnkEtS4QxySSqcQS5JhTPIJalw\nBrkkFc4gl6TCGeSSVDiDXJIKZ5BLUuEMckkqnEEuSYUzyCWpcAa5JBXOIJekwhnkklQ4g1ySCmeQ\nS1LhDHJJKpxBLkmFM8glqXAGuSQVziCXpMIZ5JJUOINckgpnkEtS4RpVDoqILcAqYBy4KjN3tuw7\nF7gJOAok8I7MPNaBWiVJ05hzRh4R64CVmbkaGAG2TjnkDmBTZq4BlgEX1l6lJGlGVZZWNgD3AGTm\nHmAoIk5o2X9mZn63uT0KvKbeEiVJs6mytLIC2NXyeLT53EGAzDwIEBE/B5wP/PlsJxsaOp5GY2lb\nxQ426l/Sr/OcnaivDoONAYaHl3W7DMAetmOyJnu4cOepW6d7WGmNfIolU5+IiOXAfcAfZeb/zPbi\nsbEX2njLCYeP1Lv0PtgYqPWcdddXh8kx7h891O1SAHs4X63js4dz67X+QX09nO1DoEqQ72NiBj7p\nZOCZyQfNZZYHgesyc1ubNUqS2lTl/yHbgE0AEXEGsC8zWz9WbgG2ZOZXO1CfJGkOc87IM3NHROyK\niB3AMWBzRFwGHAC+BvwBsDIi3tF8yd9n5h2dKliS9JMqrZFn5rVTnnq8Zfu4+sqRJM1Xb17ilSRV\nZpBLUuEMckkqnEEuSYUzyCWpcAa5JBXOIJekwhnkklQ4g1ySCmeQS1LhDHJJKpxBLkmFM8glqXAG\nuSQVziCXpMIZ5JJUOINckgpnkEtS4QxySSqcQS5JhTPIJalwBrkkFc4gl6TCGeSSVDiDXJIKZ5BL\nUuEMckkqnEEuSYUzyCWpcAa5JBXOIJekwhnkklQ4g1ySCmeQS1LhDHJJKpxBLkmFa1Q5KCK2AKuA\nceCqzNzZsm8j8FfAUeCBzPzLThQqSZrenDPyiFgHrMzM1cAIsHXKIVuB3wPWAOdHxKm1VylJmlGV\npZUNwD0AmbkHGIqIEwAi4peA5zPzO5l5DHigebwkaYFUWVpZAexqeTzafO5g8+/Rln3PAb8828mG\nh5ctmWeNL7n9mje2+9KFcc3ublcwrUFguNtFNNnD+Rts/m0PK+jB/kHne9jOxc7ZgrjtkJYktadK\nkO9jYuY96WTgmRn2ndJ8TpK0QKoE+TZgE0BEnAHsy8xDAJm5FzghIl4bEQ3gTc3jJUkLZMn4+Pic\nB0XEXwPnAMeAzcDpwIHMvDsizgFubh76pcz8aKeKlSS9XKUglyT1Ln+yU5IKZ5BLUuEq/Yh+P+j3\n2wzMMb69wHeYGB/A2zLz6YWu8ZWKiNOAe4EtmXn7lH390MPZxreXwnsYER8G1jKROzdl5pdb9hXf\nP5hzjHvpUA8XRZC33mYgIl4H3AWsbjlkK3AB8DTwcER8KTP/pQultqXC+AAuyswfLHx19YiIVwO3\nAQ/NcEjpPZxrfFBwDyPiXOC05r/R1wCPAV9uOaTo/kGlMUKHerhYllb6/TYDM46vj7wIXMw0P6fQ\nJz2ccXx94pvAJc3t/cCrI2Ip9E3/YJYxdtqimJFT820GetBs45v0iYh4LbAd+EBmFvV1pcw8AhyJ\niOl2F9/DOcY3qdgeZuZR4IfNhyNMLJ9MLjEU3z+Yc4yTOtLDxTIjn6rfbzMwdQwfBN4PrAdOY+Ju\nlf2sH3o4VV/0MCJ+l4mQu3KWw4ru3yxj7FgPF8uMvN9vMzDb+MjMz05uR8QDwK8BX1yw6jqvH3o4\nq37oYURcAFwHXJiZB1p29U3/ZhljR3u4WGbk/X6bgRnHFxEnRsTXIuJVzWPXAf/cnTI7o096OKN+\n6GFEnAh8BHhTZj7fuq9f+jfbGDvdw0Xzk539fpuBOcZ3FfB24EdMXEl/b0nrqwARcSZwC/Ba4DAT\n3274CvCf/dDDCuMruocR8S7geuDJlqe/DjzRD/2DSmPsWA8XTZBLUr9aLEsrktS3DHJJKpxBLkmF\nM8glqXAGuSQVziCXpMIZ5JJUuP8DwxUKfZWV82YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8390cd1358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare estimate, y_hat, to actually score\n",
    "plt.bar([0,1,2], y, width = 0.35, alpha=0.8)\n",
    "plt.bar([0.35,1.35,2.35], t.N.y_hat, width = 0.35, color='r', alpha=0.8)\n",
    "plt.grid(1)\n",
    "legend(['y', 'y_hat']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "1. What happens if you decrease maxiter for the Trainer? \n",
    "2. What happens if you increase maxiter for the Trainer?\n",
    "3. Why does this happen? Think about the gradient.\n",
    "4. What happens if you don't scale the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
