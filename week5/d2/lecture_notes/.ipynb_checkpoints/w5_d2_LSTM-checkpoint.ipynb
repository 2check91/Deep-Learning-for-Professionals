{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMs and Advanced Temporal Neurons\n",
    "\n",
    "(partially exerpted from cs231 lecture notes)\n",
    "\n",
    "The underlying problems with Simple RNNs are serious but the power of these systems and the opportunities they present have created a rich field of research attempting to overcome their limitations.\n",
    "\n",
    "### Addressing the Unstable Gradient Problem\n",
    "\n",
    "The unstable gradient previously mentioned can of course be fixed...if we hold the forward-propagated weights to 1. Then the network can propagate forever without trouble. The problem with this is that the neurons cannot learn from their past. Unless, we provide a way for the neuron to selectively remember and store memories. This is the fundamental principal behind the LSTM or \"long short-term memory\" neuron.\n",
    "\n",
    "LSTM memory cell\n",
    "-----\n",
    "\n",
    "![chain](./images/chain.png)\n",
    "\n",
    "\n",
    "The LSTM is an example of a 'gated' neuron, who effectively owns decision layers - layers within layers, so to speak. Each gate is a decision layer with its own activation function, weights and consideration of the input data vector. \n",
    "\n",
    "1. An input gate\n",
    "2. An output gate\n",
    "3. A keep/forget data (\"memory\") gate. \n",
    "\n",
    "We can formulate the system equations as follows:\n",
    "\n",
    "$$f_t = \\sigma_{g}(W_{f}x_t + U_{f}h_{t-1} + b_f)$$\n",
    "$$i_t = \\sigma_{g}(W_{i} x_t + U_{i} h_{t-1} + b_i)$$\n",
    "$$o_t = \\sigma_{g}(W_{o} x_t + U_{o} h_{t-1} + b_o)$$\n",
    "$$c_t = f_t \\circ c_{t-1} + i_t \\circ \\sigma_c(W_{c} x_t + U_{c} h_{t-1} + b_c)$$\n",
    "$$h_t = o_t \\circ \\sigma_h(c_t)$$\n",
    "\n",
    "\n",
    "Variables\n",
    "* $x_t$: input vector\n",
    "* $h_t$: output vector\n",
    "* $c_t$: cell state vector\n",
    "* $W$, $U$ and $b$: Present parameter matrix, temporal parameter matrix and bias vector\n",
    "* $f_t$, $i_t$ and $o_t$: gate vectors\n",
    "* $f_t$: Forget gate vector. Weight of remembering old information.\n",
    "* $i_t$: Input gate vector. Weight of acquiring new information.\n",
    "* $o_t$: Output gate vector. Output candidate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How LSTMs Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM neuron (at this point called \"cell\") stores its place within parameter space (state) in the persistent master cell state **memory** vector:\n",
    "\n",
    "$$c_t = f_t \\circ c_{t-1} + i_t \\circ \\sigma_c(W_{c} x_t + U_{c} h_{t-1} + b_c)$$\n",
    "\n",
    "\n",
    "### Step 1\n",
    "\n",
    "The first  step during prediction that the LSTM considers is **whether or not to flush its memory (for that timestep)**:\n",
    "\n",
    "![step1](./images/1step.png)\n",
    "\n",
    "Why? Consider subjects in our previous lecture: (\"Doug saw Doug saw Doug\"). For example:\n",
    "\n",
    "> Chris is my aunt. Cameron is my ...\n",
    "\n",
    "When we see a new subject, we want to forget the old subject (temporarily, perhaps)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "\n",
    "Now we update the new cell state based on what we kept from the old cell state.\n",
    "\n",
    "![step2](./images/2step.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "\n",
    "In this step we update the state vector using a context weighting:\n",
    "\n",
    "![step3](./images/3step.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "\n",
    "At last we decide to produce a prediction. At this point, we filter how much of the cell state will be output (allowing the cell to literally keep a memory in its state vector) based on output weights. Finally this is wrapped in a typical squashing function. \n",
    "\n",
    "![step4](./images/4step.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "<center><img src=\"images/variants.png\" height=\"500\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## GRUs (gated recurrent units)\n",
    "\n",
    "LSTMs offer tremendous performance, essentially curing all the weaknesses of RNNs and MLPs. However they are miserable to optimize, as you will see in the laboratory exercises. One way of addressing this is to simplify the LSTM by removing the output gate, providing an update gate (input), reset gate (forget gate) and reporting the system state as output instead as the activation.\n",
    "This makes the systemic equations:\n",
    "\n",
    "$$z_t = \\sigma_{g}(W_{z}x_t + U_{z}h_{t-1} + b_z)$$\n",
    "$$r_t = \\sigma_{g}(W_{r} x_t + U_{i} h_{t-1} + b_r)$$\n",
    "$$h_t = z_t \\circ h_{t-1} + (1-z_t) \\circ \\sigma_{h}(W_{h} x_t + U_{h}(r_{t} \\circ h_{t-1}) + b_{h})$$\n",
    "\n",
    "The update and reset gates end up becoming a scaling factor with respect to how much and what aspects of the previous state of the cell are included in the current response. The rest of the computation is performed simply with a single layer of weights held in the response, which becomes the state vector as well. (as you can see). GRUs are significantly cheaper and offer [similar performance](https://arxiv.org/pdf/1412.3555.pdf) to LSTM cells, and as you can imagine this remains a hot topic of research. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
