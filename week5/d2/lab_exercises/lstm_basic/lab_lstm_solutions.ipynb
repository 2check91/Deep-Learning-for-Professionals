{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a LSTM on the IMDB sentiment classification task.\n",
    "-------\n",
    "\n",
    "![](http://www.clipartbest.com/cliparts/di8/ykA/di8ykA4ie.jpeg)\n",
    "[Based on this code](https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".\n",
    "\n",
    "> As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.\n",
    "\n",
    "Read more about the data [here](https://keras.io/datasets/) and [here](http://ai.stanford.edu/~amaas/data/sentiment/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a sample review:\n",
    "    \n",
    "> Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Why are the review encoded as a sequence of word indexes and __not__ as strings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SOLUTION__: Computers perfer numbers over words. They don't understand \"words\" have meanings. It is faster to just learn a sequence of numbers then translate those to words after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "\n",
    "maxlen = 80  # Cut texts after this number of words (among top max_features most common words)\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, \n",
    "                                 maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test,\n",
    "                                maxlen=maxlen)\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Why do we need to pad the sequences?\n",
    "\n",
    "<br>\n",
    "<details><summary>\n",
    "Click here for a hint…\n",
    "</summary>\n",
    "Before padding, what is the shape of the review #0 and #1?\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SOLUTION__: \n",
    "\n",
    "1. Since TensorFlow unfolds our recurrent network for a given number of steps, we can only feed sequences of that shape to the network.\n",
    "\n",
    "2. We also want the input to have a fixed size so that we can represent a training batch as a single tensor of shape batch size x max length x features.\n",
    "\n",
    "[Source](http://danijar.com/variable-sequence-lengths-in-tensorflow/\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "max_features = 20000\n",
    "model.add(Embedding(max_features,\n",
    "                    128))\n",
    "\n",
    "model.add(LSTM(128, \n",
    "               dropout=0.2, \n",
    "               recurrent_dropout=0.2))\n",
    "\n",
    "model.add(Dense(1, \n",
    "                activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='SGD',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,691,713.0\n",
      "Trainable params: 2,691,713\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Summarize this model in 1 or 2 sentences:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SOLUTION__: It takes sequences of numbers (former words) embedds them into a vector, passes it through LSTM, and makes a binary prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: How does the size of the data compare to the number parameters in the model? Is this a problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SOLUTION__: There are way too many parameters for the amount of data. We are over fitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 178s - loss: 0.6926 - acc: 0.5247 - val_loss: 0.6925 - val_acc: 0.5313\n",
      "24992/25000 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 32\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "priint('Testing...')\n",
    "score, accuracy = model.evaluate(x_test, y_test,\n",
    "                                 batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.693\n",
      "Test accuracy: 0.531\n"
     ]
    }
   ],
   "source": [
    "print('Test score: {:.3}'.format(score))\n",
    "print('Test accuracy: {:.3}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: How does the model do? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SOLUTION__: AWFUL. Let it run! MOAR EPOCHS!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: What other machine learning models would fit this data? Which one would you use and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SOLUTION__:  KISS. The dataset is actually too small for LSTM to be of any advantage compared to simpler, much faster methods such as TF-IDF + LogReg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Modify any way you want to improve its performance\n",
    "\n",
    "Stanford researchers in a 2011 achieved an accuracy of 88.89%. Kaggle competition titled “Bag of Words Meets Bags of Popcorn” in late 2014 to early 2015. Accuracy was achieved above 97% with winners achieving 99%.\n",
    "\n",
    "Notes:\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SOLUTION__: Other NN models http://machinelearningmastery.com/predict-sentiment-movie-reviews-using-deep-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
