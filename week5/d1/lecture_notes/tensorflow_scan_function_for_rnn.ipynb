{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  TensorFlow Scan Function\n",
    "## Refs\n",
    "1.  https://rdipietro.github.io/tensorflow-scan-examples/  \n",
    "\n",
    "## The reason for learning about the scan function\n",
    "\n",
    "The output of an RNN comes from the repetitive application of the RNN cell's basic function mapping input and the last value of the RNN output to a new value of RNN output.  For the RNN to learn to remember things for several steps it needs to know the gradient of the output with respect to the input several stages ago.  Training RNNs is a gradient descent process, just like for other neural nets.  So to train the weights to pay attention to past data, the gradient must include information about how the weights several stages in the past would change current output values.  Calculating that can be problematic.  A simple example will illustrate.  \n",
    "\n",
    "Suppose you're using a recursion to raise w to an integer power and you want to take the derivative of the resulting function with respect to w.  Here's tensorflow code to accomplish that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 3.0] Tensor(\"gradients/AddN:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "g1 = tf.Graph()\n",
    "\n",
    "with g1.as_default():\n",
    "    \n",
    "    #define recursion    \n",
    "#def fn(X, w):\n",
    "#    print X, w\n",
    "#    return tf.matmul(w, X)\n",
    "\n",
    "    #result of recursion\n",
    "    x = tf.Variable(1.0, dtype=tf.float32, name='x')\n",
    "    \n",
    "    #w\n",
    "    w = tf.Variable(1.0, dtype=tf.float32, name='w')\n",
    "    \n",
    "    #successive application of recursion\n",
    "    wx = x*w\n",
    "    w2x = wx* w\n",
    "    w3x = w2x*w\n",
    "    #print w3x\n",
    "    \n",
    "    #calculate gradient\n",
    "    gradW = tf.gradients(w3x, [w])[0]\n",
    "\n",
    "#with tf.Session(graph=g1) as sess:\n",
    "with tf.Session(graph=g1) as sess:\n",
    "    writer = tf.train.SummaryWriter('logs/',graph=sess.graph)\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    deriv = sess.run([w3x, gradW])\n",
    "    print deriv, gradW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Use tensorboard to check out the graph of this tf program.  It's pretty simple.  Three multiplications by w in a sequence.  So it produces $w^3 x$ and takes the derivative of that function with respect to w with x=1.0.  The result gets printed, so you can see that the function $w^3 x = 1.0$ (w and x are both initialized to 1.0).  The derivative of the function is 3.0 as you'd expect.\n",
    "\n",
    "## Q\n",
    "1.  What would you have to do to calculate the derivative of $w^4 x$?\n",
    "2.  How would you approach the problem of calculating the derivative  of $w^n x$ where n can be any ingeter in tensorflow?  \n",
    "3.  Suppose the input to you tensorflow model was a list of integers fed in one at a time and your task was to return the correct derivative of w raised to that integer power.  How would you attack that?  \n",
    "4.  How does this relate to recurrent neural nets?  \n",
    "\n",
    "The code below may help you understand why this matters for RNN's.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 3.0] Tensor(\"gradients/AddN:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "g2 = tf.Graph()\n",
    "\n",
    "with g2.as_default():\n",
    "    \n",
    "    #define recursion\n",
    "    def fn(X, W):\n",
    "        return W * X\n",
    "\n",
    "    #result of recursion\n",
    "    x = tf.Variable(1.0, dtype=tf.float32, name='x')\n",
    "    \n",
    "    #w\n",
    "    w = tf.Variable(1.0, dtype=tf.float32, name='w')\n",
    "    \n",
    "    #successive application of recursion\n",
    "    wnx = x\n",
    "    for i in range(3):\n",
    "        wnx = fn(wnx, w)\n",
    "    \n",
    "    #print w3x\n",
    "    \n",
    "    #calculate gradient\n",
    "    gradW = tf.gradients(wnx, [w])[0]\n",
    "\n",
    "#with tf.Session(graph=g1) as sess:\n",
    "with tf.Session(graph=g2) as sess:\n",
    "    writer = tf.train.SummaryWriter('logs/',graph=sess.graph)\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    deriv = sess.run([wnx, gradW])\n",
    "    print deriv, gradW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q\n",
    "How is this related to the recursion in an RNN?\n",
    "\n",
    "The recursion for a simple Elman net goes something like $h_t = f(W_hh_{t-1} + W_xx_t)$, where $h_t$ is the hidden state at t and $x_t$ is the input at t.  Adjusting the weight matrices ($W_h$ and $W_x$) requires taking gradients backward in time through several recursive applications of the RNN recursion f().  If the number of recursions is fixed for some reason, then you can take the approach you've seen above.  Build a for-loop with the correct number of steps and have each step backwards laid out on the computation graph.  However, in some problems the sequences that are input for training do not have fixed lengths.  Protein folding and machine translation are both examples where each of the input examples has a different length.  For these problems, it is necessary to have a more general representation of the derivative of \"n\" recursive applications of RNN recursion.  This would be somewhat analogous to encoding the general expression for the derivative of variable raised to an integer power - $\\frac{d}{dw} w^n x = nw^{n-1}x$ instead of performing each of the multiplications as you saw in the examples above.  Then the iterative application of the recursion and its gradient can be represented as a single block on the graph - a block that incorporates the smarts to compute the derivative correctly no matter the length on the sequence.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_1:0\", shape=(1,), dtype=int32)\n",
      "Tensor(\"gradients/scan/while/mul/Enter_grad/b_acc_3:0\", dtype=float32)\n",
      "[array([5], dtype=int32), array([  2.,   4.,   8.,  16.,  32.], dtype=float32), 80.0, 5]\n",
      "[array([6], dtype=int32), array([  2.,   4.,   8.,  16.,  32.,  64.], dtype=float32), 192.0, 6]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "g3 = tf.Graph()\n",
    "with g3.as_default():\n",
    "    #define recursion\n",
    "    def fn(previous_output, current_input):\n",
    "        w = tf.Variable(2.0, name='w')\n",
    "        return w * previous_output * current_input\n",
    "    \n",
    "    inputList = tf.placeholder(tf.float32, shape=(None))\n",
    "    seqLength = tf.placeholder(tf.int32, shape=[1])\n",
    "    print(seqLength)\n",
    "    elems = inputList\n",
    "    #elems = tf.identity(elems)\n",
    "    initializer = tf.constant(1.0)\n",
    "    out = tf.scan(fn, elems, initializer=initializer)\n",
    "    \n",
    "    outShape = tf.shape(out)\n",
    "    nList = outShape[0]\n",
    "    trainables = tf.trainable_variables()\n",
    "    grad = tf.gradients(tf.slice(out, seqLength-1, [1]), trainables)[0]\n",
    "    print(grad)\n",
    "\n",
    "with tf.Session(graph=g3) as sess:\n",
    "    writer = tf.train.SummaryWriter('logs/',graph=sess.graph)\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    print(sess.run([outShape, out, grad, nList], {inputList:[1.0, 1.0, 1.0, 1.0, 1.0] , seqLength: [5] }))\n",
    "    print(sess.run([outShape, out, grad, nList], {inputList:[1.0, 1.0, 1.0, 1.0, 1.0, 1.0] , seqLength: [6]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see when the input sequence is 5 long, the derivative of the scan function output is calculating the derivative of $w^5$ and getting $5w^4$ and when it's 6 long the is being taken of $w^6$ and the calculation yields $6w^5$.  The scan function uses the input function (fn in the example above) to recursively derive the network output and recursively applies the derivative of fn to recursively define the gradients of the network output - including dependences on inputs and outputs (potentially) many steps into the past.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Shape:0\", shape=(?,), dtype=int32)\n",
      "Tensor(\"scan/TensorArrayPack:0\", dtype=float32)\n",
      "Tensor(\"gradients/scan/while/Enter_1_grad/Exit:0\", shape=(), dtype=float32)\n",
      "[array([  2.,   4.,   8.,  16.,  32.], dtype=float32), 32.0, array([5], dtype=int32), 32.0]\n",
      "[array([   64.,   128.,   256.,   512.,  1024.,  2048.], dtype=float32), 64.0, array([6], dtype=int32), 2048.0]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "g4 = tf.Graph()\n",
    "with g4.as_default():\n",
    "        \n",
    "    #define recursion\n",
    "    def fn(previous_output, current_input):\n",
    "        w = tf.Variable(2.0, name='w')\n",
    "        return w * previous_output * current_input\n",
    "    \n",
    "    \n",
    "    inputList = tf.placeholder(tf.float32, shape=(None))\n",
    "    seqLength = tf.shape(inputList)\n",
    "    #seqLength = tf.placeholder(tf.int32, shape=[1])\n",
    "    print(seqLength)\n",
    "    elems = inputList\n",
    "    #elems = tf.identity(elems)\n",
    "    #initializer = tf.constant(1.0)\n",
    "    lastOutput = tf.Variable(1.0, name='lastOutput')\n",
    "    #lastOutput = tf.constant(1.0)\n",
    "    out = tf.scan(fn, elems, initializer=lastOutput)\n",
    "    print(out)\n",
    "    \n",
    "    \n",
    "    lou = lastOutput.assign(tf.squeeze(tf.slice(out, seqLength-1, [1])))\n",
    "    trainables = tf.trainable_variables()\n",
    "    grad = tf.gradients(tf.slice(out, seqLength-1, [1]), trainables)[0]\n",
    "    print(grad)\n",
    "\n",
    "with tf.Session(graph=g4) as sess:\n",
    "    writer = tf.train.SummaryWriter('logs/',graph=sess.graph)\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    print(sess.run([out, grad, seqLength, lou], {inputList:[1.0, 1.0, 1.0, 1.0, 1.0] }))\n",
    "    print(sess.run([out, grad, seqLength, lou], {inputList:[1.0, 1.0, 1.0, 1.0, 1.0, 1.0] }))\n",
    "    #print(sess.run([outShape, out, grad, nList], {inputList:[1.0, 1.0, 1.0, 1.0, 1.0] , seqLength: [5] }))\n",
    "    #print(sess.run([outShape, out, grad, nList], {inputList:[1.0, 1.0, 1.0, 1.0, 1.0, 1.0] , seqLength: [6]}))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Shape:0\", shape=(?,), dtype=int32)\n",
      "Tensor(\"scan/while/Shape:0\", shape=(2,), dtype=int32)\n",
      "Tensor(\"scan/TensorArrayPack:0\", dtype=float32)\n",
      "Tensor(\"Squeeze:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shapes (1, 200) and () are not compatible",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-1a5a869578e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseqLength\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[0mlou\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlastOutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m     \u001b[0mtrainables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseqLength\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mike/Applications/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/variables.pyc\u001b[0m in \u001b[0;36massign\u001b[1;34m(self, value, use_locking)\u001b[0m\n\u001b[0;32m    451\u001b[0m       \u001b[0mthe\u001b[0m \u001b[0massignment\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mcompleted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m     \"\"\"\n\u001b[1;32m--> 453\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mstate_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_locking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0massign_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mike/Applications/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.pyc\u001b[0m in \u001b[0;36massign\u001b[1;34m(ref, value, validate_shape, use_locking, name)\u001b[0m\n\u001b[0;32m     38\u001b[0m   return _op_def_lib.apply_op(\"Assign\", ref=ref, value=value,\n\u001b[0;32m     39\u001b[0m                               \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m                               use_locking=use_locking, name=name)\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mike/Applications/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.pyc\u001b[0m in \u001b[0;36mapply_op\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    653\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    654\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    656\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_Restructure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_n_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mike/Applications/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[0;32m   2154\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[0;32m   2155\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2156\u001b[1;33m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2157\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2158\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mike/Applications/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   1610\u001b[0m       raise RuntimeError(\"No shape function registered for standard op: %s\"\n\u001b[0;32m   1611\u001b[0m                          % op.type)\n\u001b[1;32m-> 1612\u001b[1;33m   \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1613\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1614\u001b[0m     raise RuntimeError(\n",
      "\u001b[1;32m/home/mike/Applications/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/state_ops.pyc\u001b[0m in \u001b[0;36m_AssignShape\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;31m# but that is a sufficiently niche case that supporting it does\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;31m# not seem worthwhile.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mike/Applications/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.pyc\u001b[0m in \u001b[0;36mmerge_with\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    552\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m         raise ValueError(\"Shapes %s and %s are not compatible\" %\n\u001b[1;32m--> 554\u001b[1;33m                          (self, other))\n\u001b[0m\u001b[0;32m    555\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shapes (1, 200) and () are not compatible"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "g5 = tf.Graph()\n",
    "with g5.as_default():\n",
    "    \n",
    "    def init_weights(shape, name, glorot=False):\n",
    "        [n_inputs, n_outputs] = shape\n",
    "        init_range = tf.sqrt(6.0 / (n_inputs + n_outputs))\n",
    "        if glorot: return tf.Variable(tf.random_uniform(shape, -init_range, init_range), name=name)\n",
    "        else: return tf.Variable(tf.random_normal(shape, stddev=0.01), name=name)\n",
    "    \n",
    "    def bias_variable(shape, name):\n",
    "        initial = tf.constant(0.1, shape=shape)\n",
    "        return tf.Variable(initial, name=name)\n",
    "        \n",
    "    #define recursion\n",
    "    def fn(htm1, current_input):\n",
    "        #h_t = f(h_{t-1} W_hh + i_t W_hi + b_h)\n",
    "        #o_t = f2(h_{t-1} W_oh )\n",
    "        netDim = 200\n",
    "        inputDim = 1\n",
    "        b_h = bias_variable(shape=[1, netDim], name='b_h')\n",
    "        W_hh = init_weights(shape=[netDim, netDim], name='W_hh')\n",
    "        W_ih = init_weights(shape=[inputDim, netDim], name='W_ih')\n",
    "                \n",
    "        ht = tf.nn.elu(tf.matmul(htm1, W_hh) + tf.matmul(current_input, W_ih) + b_h)\n",
    "        print(tf.shape(ht))\n",
    "        return ht\n",
    "    \n",
    "    inputList = tf.placeholder(tf.float32, shape=(None))\n",
    "    label = tf.placeholder(tf.float32, shape=[None])\n",
    "    seqLength = tf.shape(inputList)\n",
    "    #seqLength = tf.placeholder(tf.int32, shape=[1])\n",
    "    print(seqLength)\n",
    "    elems = inputList\n",
    "    #elems = tf.identity(elems)\n",
    "    #initializer = tf.constant(1.0)\n",
    "    lastOutput = tf.Variable(tf.ones([1,200], name='lastOutput'))\n",
    "    #lastOutput = tf.constant(1.0)\n",
    "    out = tf.scan(fn, elems, initializer=lastOutput)\n",
    "    print(out)\n",
    "    \n",
    "    temp = tf.squeeze(tf.slice(out, seqLength-1, [1]))\n",
    "    print(temp)\n",
    "    lou = lastOutput.assign(temp)\n",
    "    trainables = tf.trainable_variables()\n",
    "    grad = tf.gradients(tf.slice(out, seqLength-1, [1]), trainables)[0]\n",
    "    print(grad)\n",
    "\n",
    "with tf.Session(graph=g5) as sess:\n",
    "    writer = tf.train.SummaryWriter('logs/',graph=sess.graph)\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    print(sess.run([out, grad, seqLength, lou], {inputList:[1.0, 1.0, 1.0, 1.0, 1.0], label: 5.0}))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"scan/TensorArrayPack:0\", dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Squeeze_1:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot infer num from shape <unknown>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-3bbfc55d1f54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/mike/Applications/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.pyc\u001b[0m in \u001b[0;36munpack\u001b[1;34m(value, num, name)\u001b[0m\n\u001b[0;32m    273\u001b[0m     \u001b[0mnum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnum\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot infer num from shape %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    276\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot infer num from shape <unknown>"
     ]
    }
   ],
   "source": [
    "print(tf.unpack(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
