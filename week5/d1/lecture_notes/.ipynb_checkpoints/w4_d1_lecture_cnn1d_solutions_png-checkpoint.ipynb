{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Objectives\n",
    "\n",
    "By the end of this lab, you will have\n",
    "\n",
    "- Implemented Convolution, ReLU, and Max-Pooling layers\n",
    "- Created a reusable convolutional block layer\n",
    "- Verfied the correctness of your implementations with gradient checking\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Interface\n",
    "\n",
    "Recall when implementing a layer to make it conform to the following interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def forward(self, inputs):\n",
    "        raise NotImplementedError('Forward pass not implemented!')\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        raise NotImplementedError('Backward pass not implemented!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max-Pooling Layer\n",
    "\n",
    "Consider a 1D Max-Pooling layer described by the computational graph\n",
    "\n",
    "![Simple Max Pooling Layer](images/Simple%20Max%20Pooling%20Layer.png)\n",
    "where\n",
    "\n",
    "$$\n",
    "z = \\max(\\mathbf{h}).\n",
    "$$\n",
    "\n",
    "### Questions\n",
    "\n",
    "- How many dimensions in $\\nabla_h$ will be non-zero assuming there is a unique $h_i$ such that $h_i = \\max(\\mathbf{h})$?\n",
    "- What if there are two values $h_i$ and $h_j$ such that $h_i = h_j = \\max(\\mathbf{h})$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "- Implement a 1D Max-Pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MaxPool(Layer):\n",
    "    def forward(self, h):\n",
    "        z = np.max(h)\n",
    "        self.cache = locals()\n",
    "        return z\n",
    "    \n",
    "    def backward(self, dz):\n",
    "        h, z = self.cache['h'], self.cache['z']\n",
    "        dh = np.zeros_like(h)\n",
    "        dh[h==z] = 1\n",
    "        dh = dh * dz\n",
    "        return dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU Layer\n",
    "\n",
    "Consider the ReLU layer described by the computational graph\n",
    "\n",
    "![Simple ReLU Layer](images/Simple%20ReLU%20Layer.png)\n",
    "where\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_{i} = \n",
    "\\begin{cases} \n",
    "0 & \\text{if } \\mathbf{a}_{i} \\leq 0 \\\\\n",
    "\\mathbf{a}_{i} & \\text{otherwise}\n",
    "\\end{cases}.\n",
    "$$\n",
    "\n",
    "### Questions\n",
    "\n",
    "- What will $\\nabla_a$ be if $a_i < 0$ for all $i$?\n",
    "- What if $a_i > 0$ for all $i$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "- Implement a 1D ReLU layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class ReLU(Layer):\n",
    "    def forward(self, a):\n",
    "        self.cache = locals()\n",
    "        h = copy.deepcopy(a)\n",
    "        h[a < 0] = 0        \n",
    "        return h\n",
    "    \n",
    "    def backward(self, dh):\n",
    "        a = self.cache['a']\n",
    "        da = np.ones_like(dh)\n",
    "        da[a < 0] = 0\n",
    "        dh = da * dh\n",
    "        return dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Layer\n",
    "\n",
    "Consider a 1D convolution layer with a single filter $w$ described by the computational graph\n",
    "\n",
    "![Simple Conv1D Layer](images/Simple%20Conv1D%20Layer.png)\n",
    "where $a_i = w * x_i$. Note since $w \\in \\mathbb{R}$, we are performing a 1x1 convolution. Further assume that we are only dealing with a stride of 1.\n",
    "\n",
    "### Questions\n",
    "\n",
    "- How many elements in $\\mathbf{a}$ does $x_i$ influence?\n",
    "- How many elements in $\\mathbf{a}$ does $w$ influence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "- Implement a 1D convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1D(Layer):\n",
    "    def forward(self, x, w):\n",
    "        a = x * w\n",
    "        self.cache = locals()\n",
    "        return a\n",
    "    \n",
    "    def backward(self, da):\n",
    "        x, w = self.cache['x'], self.cache['w']\n",
    "        dx, dw = w*da, np.sum(x*da)\n",
    "        return dx, dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Block\n",
    "\n",
    "- Consider a convolutional block layer described by the computational graph\n",
    "\n",
    "$$\n",
    "\\underset{w \\in \\mathbb{R}}{\\overset{\\mathbf{x} \\in \\mathbb{R}^N}{\\longrightarrow}}\n",
    "\\text{Conv}\n",
    "\\longrightarrow\n",
    "\\text{ReLU}\n",
    "\\longrightarrow\n",
    "\\text{Max Pool}\n",
    "\\overset{h \\in \\mathbb{R}}{\\longrightarrow}\n",
    "$$\n",
    "\n",
    "### Tasks\n",
    "\n",
    "- Implement a convolutional block layer in terms of Convolutional, ReLU, and Max Pool layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConvBlock(Layer):\n",
    "    def __init__(self):\n",
    "        self.conv, self.relu, self.max_pool = Conv1D(), ReLU(), MaxPool()\n",
    "        \n",
    "    def forward(self, x, w):\n",
    "        a = self.conv.forward(x, w)\n",
    "        h = self.relu.forward(a)\n",
    "        z = self.max_pool.forward(h)\n",
    "        return z\n",
    "    \n",
    "    def backward(self, dz):\n",
    "        dh = self.max_pool.backward(dz)\n",
    "        da = self.relu.backward(dh)\n",
    "        dx, dw = self.conv.backward(da)\n",
    "        return dx, dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Your Implementation\n",
    "\n",
    "An indispensible tool to check your backpropagation code is *gradient checking*. Gradient checking works by\n",
    "\n",
    "1. Running your backward pass to compute the gradients\n",
    "2. Approximating the gradients with finite differences\n",
    "3. Compares these two values and returns success if they are close and fails otherwise\n",
    "\n",
    "### Tasks\n",
    "\n",
    "- Run the following code cell to gradient check your convolutional block\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- The code will create a vector $x$ of five random numbers and a random filter $w$. It approxiates $\\nabla{x}$ and $\\nabla{w}$ and compares those values against the values of $\\nabla{w}$ and $\\nabla{x}$ your `ConvBlock.backward()` method returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check on param #0 PASSED with frobenius norm difference 2.6554869414496807e-12!\n",
      "Gradient check on param #1 PASSED with frobenius norm difference 7.949196856316121e-14!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from lib.checking import gradient_check\n",
    "\n",
    "x, w = np.random.randn(5), np.random.randn()\n",
    "params = [x, w]\n",
    "\n",
    "conv_block = ConvBlock()\n",
    "gradient_check(conv_block.forward, conv_block.backward, *params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Activities\n",
    "\n",
    "- Implement a 1D convolution layer with support for multiple scalar filters\n",
    "- Implement a 1D convolution layer with one filter which is a vector\n",
    "- Implement a 1D convolution layer with a set of filters which are vectors\n",
    "- Implement a 2D convolution layer\n",
    "- Implement a max-pooling layer which supports local maxes\n",
    "- Implement a 2D max-pooling layer\n",
    "- Generalize your code to support minibatches\n",
    "- Implement a simple trainer class with an SGD optimizer and optimize a CNN on MNIST"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
